{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_paper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tinynja/Sarsa-phi-EB/blob/main/rl_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0ztbgbOZ-uJ",
        "outputId": "73195fe5-6922-4552-97cd-ebb737a31945"
      },
      "source": [
        "# install dependencies\n",
        "!pip install torch torchvision pyvirtualdisplay matplotlib seaborn pandas numpy pathlib gym\n",
        "!sudo apt-get install xvfb\n",
        "\n",
        "# Run this cell\n",
        "\n",
        "# type hinting \n",
        "from typing import Sequence, Tuple, Dict, Any, Optional\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# torch stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "from torch import optim\n",
        "\n",
        "# env\n",
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "# data manipulation, colab dispaly, and plotting\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# misc util\n",
        "import random, glob, base64, itertools\n",
        "from pathlib import Path\n",
        "from pprint import pprint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-2.2-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.9 [784 kB]\n",
            "Fetched 784 kB in 1s (643 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.9_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.9) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqiqcC4RVWq7"
      },
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2017-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor\n",
        "from tqdm import tqdm\n",
        "\n",
        "#######################################################################\n",
        "# Following are some utilities for tile coding from Rich.\n",
        "# To make each file self-contained, I copied them from\n",
        "# http://incompleteideas.net/tiles/tiles3.py-remove\n",
        "# with some naming convention changes\n",
        "#\n",
        "# Tile coding starts\n",
        "class IHT:\n",
        "    \"Structure to handle collisions\"\n",
        "    def __init__(self, size_val):\n",
        "        self.size = size_val\n",
        "        self.overfull_count = 0\n",
        "        self.dictionary = {}\n",
        "\n",
        "    def count(self):\n",
        "        return len(self.dictionary)\n",
        "\n",
        "    def full(self):\n",
        "        return len(self.dictionary) >= self.size\n",
        "\n",
        "    def get_index(self, obj, read_only=False):\n",
        "        d = self.dictionary\n",
        "        if obj in d:\n",
        "            return d[obj]\n",
        "        elif read_only:\n",
        "            return None\n",
        "        size = self.size\n",
        "        count = self.count()\n",
        "        if count >= size:\n",
        "            if self.overfull_count == 0: print('IHT full, starting to allow collisions')\n",
        "            self.overfull_count += 1\n",
        "            return hash(obj) % self.size\n",
        "        else:\n",
        "            d[obj] = count\n",
        "            return count\n",
        "\n",
        "def hash_coords(coordinates, m, read_only=False):\n",
        "    if isinstance(m, IHT): return m.get_index(tuple(coordinates), read_only)\n",
        "    if isinstance(m, int): return hash(tuple(coordinates)) % m\n",
        "    if m is None: return coordinates\n",
        "\n",
        "def tiles(iht_or_size, num_tilings, floats, ints=None, read_only=False):\n",
        "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
        "    if ints is None:\n",
        "        ints = []\n",
        "    qfloats = [floor(f * num_tilings) for f in floats]\n",
        "    tiles = []\n",
        "    for tiling in range(num_tilings):\n",
        "        tilingX2 = tiling * 2\n",
        "        coords = [tiling]\n",
        "        b = tiling\n",
        "        for q in qfloats:\n",
        "            coords.append((q + b) // num_tilings)\n",
        "            b += tilingX2\n",
        "        coords.extend(ints)\n",
        "        tiles.append(hash_coords(coords, iht_or_size, read_only))\n",
        "    return tiles\n",
        "# Tile coding ends\n",
        "#######################################################################\n",
        "\n",
        "# all possible actions\n",
        "env = BreakoutEnv(Breakout)\n",
        "ACTIONS = range(4)\n",
        "\n",
        "# bound for position and velocity\n",
        "POSITION_MIN = -1.2\n",
        "POSITION_MAX = 0.5\n",
        "VELOCITY_MIN = -0.07\n",
        "VELOCITY_MAX = 0.07\n",
        "\n",
        "# discount is always 1.0 in these experiments\n",
        "DISCOUNT = 1.0\n",
        "\n",
        "# use optimistic initial value, so it's ok to set epsilon to 0\n",
        "EPSILON = 0\n",
        "\n",
        "# maximum steps per episode\n",
        "STEP_LIMIT = 5000\n",
        "\n",
        "\n",
        "# get action at @position and @velocity based on epsilon greedy policy and @valueFunction  #########################    use our own get_action. modified it, may work as intended\n",
        "def get_action(observation, valueFunction):\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "        return np.random.choice(ACTIONS)\n",
        "    values = []\n",
        "    for action in ACTIONS:\n",
        "        values.append(valueFunction.value(observation))  \n",
        "    return np.argmax(values) - 1\n",
        "\n",
        "\n",
        "\n",
        "# replacing trace update rule\n",
        "# @trace: old trace (will be modified)\n",
        "# @activeTiles: current active tile indices\n",
        "# @lam: lambda\n",
        "# @return: new trace for convenience\n",
        "def replacing_trace(trace, activeTiles, lam):\n",
        "    active = np.in1d(np.arange(len(trace)), activeTiles)\n",
        "    trace[active] = 1\n",
        "    trace[~active] *= lam * DISCOUNT\n",
        "    return trace\n",
        "\n",
        "\n",
        "\n",
        "# wrapper class for Sarsa(lambda)\n",
        "class Sarsa:\n",
        "    # In this example I use the tiling software instead of implementing standard tiling by myself\n",
        "    # One important thing is that tiling is only a map from (state, action) to a series of indices\n",
        "    # It doesn't matter whether the indices have meaning, only if this map satisfy some property\n",
        "    # View the following webpage for more information\n",
        "    # http://incompleteideas.net/sutton/tiles/tiles3.html\n",
        "    # @maxSize: the maximum # of indices\n",
        "    #the hashing is a lfa?\n",
        "    def __init__(self, step_size, lam, trace_update=replacing_trace, num_of_tilings=8, max_size=2048):\n",
        "        self.max_size = max_size\n",
        "        self.num_of_tilings = num_of_tilings\n",
        "        self.trace_update = trace_update\n",
        "        self.lam = lam\n",
        "\n",
        "        # divide step size equally to each tiling\n",
        "        self.step_size = step_size / num_of_tilings\n",
        "\n",
        "        self.hash_table = IHT(max_size)\n",
        "\n",
        "        # weight for each tile\n",
        "        self.weights = np.zeros(max_size) #max size is the number of features?\n",
        "\n",
        "        # trace for each tile\n",
        "        self.trace = np.zeros(max_size)\n",
        "\n",
        "        # position and velocity needs scaling to satisfy the tile software\n",
        "        self.position_scale = self.num_of_tilings / (POSITION_MAX - POSITION_MIN)\n",
        "        self.velocity_scale = self.num_of_tilings / (VELOCITY_MAX - VELOCITY_MIN)\n",
        "\n",
        "    # get indices of active tiles for given state and action\n",
        "    def get_active_tiles(self, position, velocity, action):\n",
        "        # I think positionScale * (position - position_min) would be a good normalization.\n",
        "        # However positionScale * position_min is a constant, so it's ok to ignore it.\n",
        "        active_tiles = tiles(self.hash_table, self.num_of_tilings,\n",
        "                            [self.position_scale * position, self.velocity_scale * velocity],\n",
        "                            [action])\n",
        "        return active_tiles\n",
        "\n",
        "    # estimate the value of given state and action\n",
        "    def value(self, observation):\n",
        "        active_tiles = np.nonzero(observation)\n",
        "        return np.sum(self.weights[active_tiles])\n",
        "\n",
        "    # learn with given state, action and target\n",
        "    def learn(self, observation, target):\n",
        "        active_tiles = np.nonzero(observation)\n",
        "        estimation = np.sum(self.weights[active_tiles])\n",
        "        delta = target - estimation\n",
        "        if self.trace_update == replacing_trace:\n",
        "            self.trace_update(self.trace, active_tiles, self.lam)\n",
        "        else:\n",
        "            raise Exception('Unexpected Trace Type')\n",
        "        self.weights += self.step_size * delta * self.trace\n",
        "\n",
        "\n",
        "# play Mountain Car for one episode based on given method @evaluator\n",
        "# @return: total steps in this episode\n",
        "def play(evaluator, env):\n",
        "\n",
        "    action = random.choice(ACTIONS)\n",
        "    steps = 0\n",
        "    while True:\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        next_action = get_action(next_observation, evaluator)    #########################    use our own get_action  ??? modified it, may work as intented\n",
        "        steps += 1\n",
        "        target = reward + DISCOUNT * evaluator.value(next_observation)          ############# use our own value function ??? modified it, may work as intented\n",
        "        evaluator.learn(observation, target)\n",
        "        observation = next_observation\n",
        "        action = next_action\n",
        "        if done:\n",
        "            break\n",
        "        if steps >= STEP_LIMIT:\n",
        "            print('Step Limit Exceeded!')\n",
        "            break\n",
        "    return steps\n",
        "\n",
        "# figure 12.10, effect of the lambda and alpha on early performance of Sarsa(lambda)\n",
        "def figure_12_10():\n",
        "    runs = 30\n",
        "    episodes = 50\n",
        "    alphas = np.arange(1, 8) / 4.0\n",
        "    lams = [0.99, 0.95, 0.5, 0]\n",
        "\n",
        "    steps = np.zeros((len(lams), len(alphas), runs, episodes))\n",
        "    for lamInd, lam in enumerate(lams):\n",
        "        for alphaInd, alpha in enumerate(alphas):\n",
        "            for run in tqdm(range(runs)):\n",
        "                evaluator = Sarsa(alpha, lam, replacing_trace)\n",
        "                for ep in range(episodes):\n",
        "                    step = play(evaluator, env)\n",
        "                    steps[lamInd, alphaInd, run, ep] = step\n",
        "\n",
        "    # average over episodes\n",
        "    steps = np.mean(steps, axis=3)\n",
        "\n",
        "    # average over runs\n",
        "    steps = np.mean(steps, axis=2)\n",
        "\n",
        "    for lamInd, lam in enumerate(lams):\n",
        "        plt.plot(alphas, steps[lamInd, :], label='lambda = %s' % (str(lam)))\n",
        "    plt.xlabel('alpha * # of tilings (8)')\n",
        "    plt.ylabel('averaged steps per episode')\n",
        "    plt.ylim([180, 300])\n",
        "    plt.legend()\n",
        "\n",
        "    plt.savefig('figure_12_10.png')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    figure_12_10()\n",
        "    figure_12_11()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMeWJRRueo1t"
      },
      "source": [
        "class BaseAgent:\n",
        "  \"\"\" The base agent class function.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    #nothing for now\n",
        "    self.gamma = 1\n",
        "    self.features = 3\n",
        "    self.rhos = np.ones(self.features) #stores the rho_i values\n",
        "\n",
        "\n",
        "  def takeAction(self, t):\n",
        "    phis = [[0,1,0],[0,1,0],[0,1,0],[1,0,1]]\n",
        "    return phis[t]\n",
        "\n",
        "\n",
        "  def updateRho_i(self, counts, t):\n",
        "    M = self.features\n",
        "    for i in range(M): #M is the number of features of phi\n",
        "      counts[i] += 1 #since we add phi to the seen states, all the counts are increased by one for t+1\n",
        "      self.rhos[i] = (counts[i]+0.5)/(t+1)\n",
        "    return 0\n",
        "\n",
        "\n",
        "  def PHI_EB(self, beta, t_end, evaluator, env):\n",
        "    t = 0\n",
        "    M = self.features #number of features\n",
        "    counts = np.zeros(M)\n",
        "    states = np.zeros((t_end,M)) #stores the previous phis for all timesteps\n",
        "\n",
        "    action = np.random.choice(range(4))\n",
        "    old_phi = env.observe()\n",
        "\n",
        "    while t < t_end:\n",
        "      #observe phi(s), reward\n",
        "      phi, reward, done, info = env.step(action)\n",
        "      if done:\n",
        "        break\n",
        "      next_action = get_action(phi, evaluator)\n",
        "      #phi = self.takeAction(t) # dummy vector until we can generate episodes\n",
        "      #reward = 1\n",
        "      #print(states)\n",
        "      #print(phi)\n",
        "      \n",
        "      #compute rho_t(phi) (feature visit-density)\n",
        "      if t > 0:\n",
        "        rho_t = 1\n",
        "        for i in range(M):\n",
        "          counts[i] = 0\n",
        "          for step in range(t):\n",
        "            if phi[i] == states[step,i]:\n",
        "              counts[i] += 1\n",
        "          self.rhos[i] = (counts[i]+0.5)/(t+1)\n",
        "          rho_t = rho_t*self.rhos[i]\n",
        "      else:\n",
        "        rho_t = 0.5**M\n",
        "      print('rho_t: '+ str(rho_t))\n",
        "\n",
        "      #update all rho_i with observed phi\n",
        "      states[t] = phi\n",
        "      self.updateRho_i(counts, t+1)\n",
        "      \n",
        "      #compute rho_t+1(phi)\n",
        "      new_rho_t = 1\n",
        "      for i in range(M):\n",
        "        new_rho_t = new_rho_t*self.rhos[i]\n",
        "\n",
        "      #compute Nhat_t(s)\n",
        "      Nhat_t = rho_t*(1-new_rho_t)/(new_rho_t-rho_t)\n",
        "\n",
        "      #compute R(s,a) (empirical reward)\n",
        "      explorationBonus = beta/np.sqrt(Nhat_t)\n",
        "\n",
        "      reward = reward + explorationBonus\n",
        "      print(explorationBonus)\n",
        "      #pass phi(s) and reward to RL algo to update theta_t\n",
        "      target = reward + self.gamma * evaluator.value(phi)          ############# use our own value function ??? modified it, may work as intented\n",
        "      evaluator.learn(old_phi), target)\n",
        "\n",
        "      old_phi = phi\n",
        "      action = next_action\n",
        "      \n",
        "      theta_end = 0\n",
        "\n",
        "      t += 1\n",
        "\n",
        "    return theta_end\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf6Wzc-9sLqq"
      },
      "source": [
        "import torch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN4eP5AivJGr"
      },
      "source": [
        "# Optimized by Amine\n",
        "class SarsaPhiEB:\n",
        "    def __init__(self, env, gamma=1, beta=1):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "\n",
        "        # self.rhos = np.ones(self.features) #stores the rho_i values\n",
        "\n",
        "    def updateRho_i(self, counts, t):\n",
        "        M = self.features\n",
        "        for i in range(M): #M is the number of features of phi\n",
        "            counts[i] += 1 #since we add phi to the seen states, all the counts are increased by one for t+1\n",
        "            self.rhos[i] = (counts[i]+0.5)/(t+1)\n",
        "        return 0\n",
        "\n",
        "    def generate_action(self, observation):\n",
        "        pass\n",
        "\n",
        "    def learn_episode(self):\n",
        "        t = 0\n",
        "        counts = torch.zeros_like(self.env._observe())\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = \n",
        "            observation, reward, done, info = self.env.step()\n",
        "        #observe phi(s), reward\n",
        "        phi = self.takeAction(t) # dummy vector until we can generate episodes\n",
        "        reward = 1\n",
        "        print(states)\n",
        "        print(phi)\n",
        "        \n",
        "        #compute rho_t(phi) (feature visit-density)\n",
        "        if t > 0:\n",
        "            rho_t = 1\n",
        "            for i in range(M):\n",
        "            counts[i] = 0\n",
        "            for step in range(t):\n",
        "                if phi[i] == states[step,i]:\n",
        "                counts[i] += 1\n",
        "            self.rhos[i] = (counts[i]+0.5)/(t+1)\n",
        "            rho_t = rho_t*self.rhos[i]\n",
        "        else:\n",
        "            rho_t = 0.5**M\n",
        "        print('rho_t: '+ str(rho_t))\n",
        "\n",
        "        #update all rho_i with observed phi\n",
        "        states[t] = phi\n",
        "        self.updateRho_i(counts, t+1)\n",
        "        \n",
        "        #compute rho_t+1(phi)\n",
        "        new_rho_t = 1\n",
        "        for i in range(M):\n",
        "            new_rho_t = new_rho_t*self.rhos[i]\n",
        "\n",
        "        #compute Nhat_t(s)\n",
        "        Nhat_t = rho_t*(1-new_rho_t)/(new_rho_t-rho_t)\n",
        "\n",
        "        #compute R(s,a) (empirical reward)\n",
        "        explorationBonus = beta/np.sqrt(Nhat_t)\n",
        "\n",
        "        reward = reward + explorationBonus\n",
        "        print(explorationBonus)\n",
        "        #pass phi(s) and reward to RL algo to update theta_t\n",
        "        \n",
        "        theta_end = 0\n",
        "\n",
        "        t += 1\n",
        "\n",
        "        return theta_end\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qodJRQWdHf-x",
        "outputId": "ed40e7b8-3520-4cf6-f92d-62b3c5ad9130"
      },
      "source": [
        "#testing\n",
        "testAgent = BaseAgent()\n",
        "testAgent.PHI_EB(1,4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "[0, 1, 0]\n",
            "rho_t: 0.125\n",
            "2.026846838838127\n",
            "[[0. 1. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "[0, 1, 0]\n",
            "rho_t: 0.421875\n",
            "0.9393491802183486\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "[0, 1, 0]\n",
            "rho_t: 0.5787037037037038\n",
            "0.6910415772863893\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 0.]]\n",
            "[1, 0, 1]\n",
            "rho_t: 0.001953125\n",
            "3.630407155555409\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXc82KiGKwbx"
      },
      "source": [
        "print('Hello Bob!')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}