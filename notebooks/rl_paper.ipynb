{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rl_paper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tinynja/Sarsa-phi-EB/blob/main/notebooks/rl_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0ztbgbOZ-uJ",
        "outputId": "05d7690d-816a-4692-feab-350be81d719e"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/Tinynja/Sarsa-phi-EB/master/notebooks/ALE_Framework_Tests.ipynb\n",
        "\n",
        "%run ALE_Framework_Tests.ipynb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sarsa-phi-EB'...\n",
            "remote: Enumerating objects: 326, done.\u001b[K\n",
            "remote: Counting objects: 100% (326/326), done.\u001b[K\n",
            "remote: Compressing objects: 100% (275/275), done.\u001b[K\n",
            "remote: Total 326 (delta 106), reused 204 (delta 39), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (326/326), 761.46 KiB | 12.69 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      chopper_command ROMS/Chopper Command (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              solaris ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 hero       ROMS/H.E.R.O. (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      elevator_action ROMS/Elevator Action (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               enduro         ROMS/Enduro (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          road_runner    ROMS/Road Runner (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            blackjack ROMS/Blackjack - Black Jack (Gambling) (Paddle) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               zaxxon         ROMS/Zaxxon (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              turmoil        ROMS/Turmoil (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       name_this_game ROMS/Name This Game (Guardians of Treasure) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             pitfall2 ROMS/Pitfall II - Lost Caverns (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               tennis ROMS/Tennis - Le Tennis (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         darkchambers ROMS/Dark Chambers (Dungeon, Dungeon Masters) (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         demon_attack ROMS/Demon Attack (Death from Above) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             trondead ROMS/TRON - Deadly Discs (TRON Joystick) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             entombed ROMS/Entombed (Maze Chase, Pharaoh's Tomb, Zombie) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               casino ROMS/Casino - Poker Plus (Paddle) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert         ROMS/Q-bert (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m     human_cannonball ROMS/Human Cannonball - Cannon Man (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       journey_escape ROMS/Journey Escape (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            king_kong      ROMS/King Kong (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              phoenix        ROMS/Phoenix (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             robotank ROMS/Robot Tank (Robotank) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             breakout ROMS/Breakout - Breakaway IV (Paddle) (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert        ROMS/Q. Bert (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            centipede      ROMS/Centipede (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            adventure      ROMS/Adventure (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pooyan         ROMS/Pooyan (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            riverraid     ROMS/River Raid (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                mr_do        ROMS/Mr. Do! (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       video_checkers ROMS/Video Checkers - Checkers - Atari Video Checkers (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        wizard_of_wor  ROMS/Wizard of Wor (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           mario_bros    ROMS/Mario Bros. (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          word_zapper ROMS/Word Zapper (Word Grabber) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               amidar         ROMS/Amidar (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               kaboom ROMS/Kaboom! (Paddle) (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              berzerk        ROMS/Berzerk (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         yars_revenge ROMS/Yars' Revenge (Time Freeze) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              koolaid ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       miniature_golf ROMS/Miniature Golf - Arcade Golf (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              venture        ROMS/Venture (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             galaxian       ROMS/Galaxian (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 klax           ROMS/Klax (1991).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          star_gunner     ROMS/Stargunner (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            atlantis2    ROMS/Atlantis II (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           backgammon ROMS/Backgammon (Paddle) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            up_n_down     ROMS/Up 'n Down (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             seaquest       ROMS/Seaquest (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            ms_pacman    ROMS/Ms. Pac-Man (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           beam_rider      ROMS/Beamrider (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               boxing ROMS/Boxing - La Boxe (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        crazy_climber  ROMS/Crazy Climber (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       tic_tac_toe_3d ROMS/3-D Tic-Tac-Toe (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m    montezuma_revenge ROMS/Montezuma's Revenge - Featuring Panama Joe (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             crossbow       ROMS/Crossbow (1988).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      keystone_kapers ROMS/Keystone Kapers - Raueber und Gendarm (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           bank_heist ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball ROMS/Video Pinball - Arcade Pinball (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                   et ROMS/E.T. - The Extra-Terrestrial (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math ROMS/Fun with Numbers (AKA Basic Math) (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              freeway        ROMS/Freeway (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            frostbite      ROMS/Frostbite (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           earthworld ROMS/SwordQuest - EarthWorld (Adventure I, SwordQuest I - EarthWorld) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         lost_luggage ROMS/Lost Luggage (Airport Mayhem) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       kung_fu_master ROMS/Kung-Fu Master (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              pitfall ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            jamesbond ROMS/James Bond 007 (James Bond Agent 007) (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              othello        ROMS/Othello (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             superman       ROMS/Superman (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             gravitar       ROMS/Gravitar (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            space_war ROMS/Space War - Space Star (32 in 1) (1988).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pacman        ROMS/Pac-Man (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix ROMS/Asterix (AKA Taz) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball ROMS/Pinball (AKA Video Pinball) (Zellers).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong    ROMS/Donkey Kong (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              assault ROMS/Assault (AKA Sky Alien) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              bowling        ROMS/Bowling (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math ROMS/Basic Math - Math (Math Pack) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             carnival       ROMS/Carnival (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           time_pilot     ROMS/Time Pilot (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               gopher ROMS/Gopher (Gopher Attack) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong    ROMS/Donkey Kong (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            tutankham      ROMS/Tutankham (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                krull          ROMS/Krull (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              frogger        ROMS/Frogger (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix ROMS/Asterix (AKA Taz) (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                alien          ROMS/Alien (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             air_raid ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        fishing_derby  ROMS/Fishing Derby (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        haunted_house ROMS/Haunted House (Mystery Mansion, Graves' Manor, Nightmare Manor) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 pong ROMS/Video Olympics - Pong Sports (Paddle) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           ice_hockey ROMS/Ice Hockey - Le Hockey Sur Glace (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          private_eye    ROMS/Private Eye (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             defender       ROMS/Defender (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             atlantis ROMS/Atlantis (Lost City of Atlantis) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           videochess ROMS/Video Chess (Computer Chess) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            videocube ROMS/Atari Video Cube (Atari Cube, Video Cube) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       space_invaders ROMS/Space Invaders (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              hangman ROMS/Hangman - Spelling (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          laser_gates ROMS/Laser Gates (AKA Innerspace) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               skiing ROMS/Skiing - Le Ski (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          battle_zone     ROMS/Battlezone (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          double_dunk ROMS/Double Dunk (Super Basketball) (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         flag_capture ROMS/Flag Capture - Capture (Capture the Flag) (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert         ROMS/Q-bert (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             kangaroo       ROMS/Kangaroo (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         sir_lancelot   ROMS/Sir Lancelot (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround ROMS/Surround - Chase (Blockade) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            asteroids      ROMS/Asteroids (1981).bin\n",
            "\n",
            "\n",
            "\n",
            "Imported 110 / 110 ROMs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for external in metadata.entry_points().get(self.group, []):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped background regeneration.\n",
            "Skipped displaying stored backgrounds to reduce ouptuts.\n",
            "Skipped manual test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sarsa phi eb"
      ],
      "metadata": {
        "id": "T3DHkXGnN0pt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqiqcC4RVWq7"
      },
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2017-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "from math import floor\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# all possible actions\n",
        "ACTIONS = range(4)\n",
        "\n",
        "# discount is always 1.0 in these experiments\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "# use optimistic initial value, so it's ok to set epsilon to 0\n",
        "EPSILON = 0.01\n",
        "\n",
        "# maximum steps per episode\n",
        "STEP_LIMIT = 5000\n",
        "\n",
        "\n",
        "# get action at @position and @velocity based on epsilon greedy policy and @valueFunction  #########################    use our own get_action. modified it, may work as intended\n",
        "def get_action(observation, valueFunction):\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "        return np.random.choice(ACTIONS)\n",
        "    values = []\n",
        "    for action in ACTIONS:\n",
        "        values.append(valueFunction.value(observation))  \n",
        "    maxi = np.max(values)\n",
        "    bestactions = np.where(values==maxi,1.0,0.0)\n",
        "    for i in range(len(ACTIONS)):\n",
        "      bestactions[i] = bestactions[i]*np.random.uniform()\n",
        "    action = np.argmax(bestactions)\n",
        "    return action\n",
        "\n",
        "\n",
        "\n",
        "# replacing trace update rule\n",
        "# @trace: old trace (will be modified)\n",
        "# @activeTiles: current active tile indices\n",
        "# @lam: lambda\n",
        "# @return: new trace for convenience\n",
        "def replacing_trace(trace, activeTiles, lam):\n",
        "    active = (torch.arange(len(trace)).to(device)[None,...] == activeTiles.flatten()[...,None]).any(0)\n",
        "    trace[active] = 1\n",
        "    trace[~active] *= lam * DISCOUNT\n",
        "    return trace\n",
        "\n",
        "\n",
        "\n",
        "# wrapper class for Sarsa(lambda)\n",
        "class Sarsa:\n",
        "    # In this example I use the tiling software instead of implementing standard tiling by myself\n",
        "    # One important thing is that tiling is only a map from (state, action) to a series of indices\n",
        "    # It doesn't matter whether the indices have meaning, only if this map satisfy some property\n",
        "    # View the following webpage for more information\n",
        "    # http://incompleteideas.net/sutton/tiles/tiles3.html\n",
        "    # @maxSize: the maximum # of indices\n",
        "    #the hashing is a lfa?\n",
        "    def __init__(self, step_size, lam, trace_update=replacing_trace, max_size=28672, initial_weights=0):\n",
        "        self.max_size = max_size\n",
        "        self.trace_update = trace_update\n",
        "        self.lam = lam\n",
        "\n",
        "        # divide step size equally to each tiling\n",
        "        self.step_size = step_size/10\n",
        "\n",
        "        # weight for each tile\n",
        "        if initial_weights == 0:\n",
        "          self.weights =torch.zeros(max_size).to(device) #max size is the number of features?\n",
        "        else:\n",
        "          self.weights = initial_weights.to(device)\n",
        "\n",
        "        # trace for each tile\n",
        "        self.trace = torch.zeros(max_size).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # estimate the value of given state and action\n",
        "    def value(self, observation):\n",
        "        active_tiles = torch.nonzero(observation)\n",
        "        return self.weights[active_tiles].sum()\n",
        "\n",
        "    # learn with given state, action and target\n",
        "    def learn(self, observation, target):\n",
        "        active_tiles = torch.nonzero(observation)\n",
        "        estimation = self.weights[active_tiles].sum()\n",
        "        delta = target - estimation\n",
        "        #print('estimation array: ' + str(self.weights[active_tiles]))\n",
        "        print('estimation: ' + str(self.weights[active_tiles].sum()))\n",
        "        if self.trace_update == replacing_trace:\n",
        "            self.trace_update(self.trace, active_tiles, self.lam)\n",
        "        else:\n",
        "            raise Exception('Unexpected Trace Type')\n",
        "        self.weights += self.step_size * delta * self.trace\n",
        "        print('delta: ' + str(delta))\n",
        "        print('weights: ' +  str(self.weights))\n",
        "\n",
        "\n",
        "# play Mountain Car for one episode based on given method @evaluator\n",
        "# @return: total steps in this episode\n",
        "def play(evaluator, env):\n",
        "\n",
        "    action = random.choice(ACTIONS)\n",
        "    steps = 0\n",
        "    while True:\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        next_action = get_action(next_observation, evaluator)    #########################    use our own get_action  ??? modified it, may work as intented\n",
        "        steps += 1\n",
        "        target = reward + DISCOUNT * evaluator.value(next_observation)          ############# use our own value function ??? modified it, may work as intented\n",
        "        evaluator.learn(observation, target)\n",
        "        observation = next_observation\n",
        "        action = next_action\n",
        "        if done:\n",
        "            break\n",
        "        if steps >= STEP_LIMIT:\n",
        "            print('Step Limit Exceeded!')\n",
        "            break\n",
        "    return steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMeWJRRueo1t"
      },
      "source": [
        "class BaseAgent:\n",
        "  \"\"\" The base agent class function.\n",
        "  \"\"\"\n",
        "  def __init__(self, nb_features=28672):\n",
        "    #nothing for now\n",
        "    self.gamma = 1\n",
        "    self.features = nb_features\n",
        "    self.rhos = torch.ones(self.features).to(device) #stores the rho_i values\n",
        "\n",
        "\n",
        "  def takeAction(self, t):\n",
        "    phis = [[0,1,0],[0,1,0],[0,1,0],[1,0,1]]\n",
        "    return phis[t]\n",
        "\n",
        "\n",
        "  def updateRho_i(self, counts, t):\n",
        "    M = self.features\n",
        "    self.rhos = (counts+1.5)/(t+1)\n",
        "    return 0\n",
        "\n",
        "\n",
        "  def PHI_EB(self, evaluator, env, beta=0.05, t_end=200):\n",
        "    t = 0\n",
        "    M = self.features #number of features\n",
        "    counts = torch.zeros(M).to(device)\n",
        "    states = torch.zeros(t_end,M).to(device) #stores the previous phis for all timesteps\n",
        "\n",
        "    action = 1 #starting the game for the agent on the first game\n",
        "    old_phi = env._observe()\n",
        "    print('starting iterations')\n",
        "    print('rhos: ' + str(self.rhos))\n",
        "    while t < t_end:\n",
        "      print(\"Iteration #{}\".format(t))\n",
        "      #observe phi(s), reward\n",
        "      phi, reward, done, info = env.step(action)\n",
        "      print('--------------------------------------------------------------')\n",
        "      print('took action: ', env.action_space[action])\n",
        "      next_action = get_action(phi, evaluator)\n",
        "      print('phi: ' + str(phi))\n",
        "      \n",
        "      #compute rho_t(phi) (feature visit-density)\n",
        "      if t > 0:\n",
        "        counts = (phi==states[0:t]).sum(0)\n",
        "        # print(counts)\n",
        "        self.rhos = (counts+0.5)/(t+1)\n",
        "        print('rhos: ' + str(self.rhos))\n",
        "        rho_t = torch.prod(self.rhos)\n",
        "      else:\n",
        "        rho_t = 0.5**M\n",
        "      print('rho_t ' + str(rho_t))\n",
        "      #update all rho_i with observed phi\n",
        "      states[t] = phi\n",
        "      self.updateRho_i(counts, t+1)\n",
        "      print('min rho_i_t: ' + str(min(self.rhos)))\n",
        "      \n",
        "      #compute rho_t+1(phi)\n",
        "      new_rho_t = 1\n",
        "      for i in range(M):\n",
        "        new_rho_t = new_rho_t*self.rhos[i]\n",
        "      if new_rho_t <= 1e-323: #this is to avoid division by zero, might need to be tweaked\n",
        "        new_rho_t = 1e-323\n",
        "      print('new_rho_t ' + str(new_rho_t))\n",
        "\n",
        "      #compute Nhat_t(s)\n",
        "      Nhat_t = rho_t*(1-new_rho_t)/(new_rho_t-rho_t)\n",
        "      print('Nhat_t: ',   Nhat_t)\n",
        "      if Nhat_t <= 1e-323: #this is to avoid division by zero again, might need to be tweaked\n",
        "        Nhat_t = torch.tensor([1e-323]).to(device)\n",
        "\n",
        "      #compute R(s,a) (empirical reward)\n",
        "      explorationBonus = beta/torch.sqrt(Nhat_t)\n",
        "      if torch.isnan(explorationBonus) or explorationBonus >= 1e3:\n",
        "        explorationBonus = 1e3\n",
        "\n",
        "      reward = reward + explorationBonus\n",
        "      print('reward: ',reward)\n",
        "\n",
        "      print('state value: ' + str(evaluator.value(phi)))\n",
        "      #pass phi(s) and reward to RL algo to update theta_t\n",
        "      target = reward + self.gamma * evaluator.value(phi)          ############# use our own value function ??? modified it, may work as intented\n",
        "      evaluator.learn(old_phi, target)\n",
        "\n",
        "      if done:\n",
        "        #break\n",
        "        env.reset()\n",
        "        action = 1\n",
        "        old_phi = env._observe()\n",
        "        print('episode ended on step ', t, 'starting a new one')\n",
        "      else:\n",
        "        old_phi = phi\n",
        "        action = next_action\n",
        "\n",
        "      t += 1\n",
        "\n",
        "    return evaluator.weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf6Wzc-9sLqq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b3c784bb-c822-413d-8217-153bb57fbf0e"
      },
      "source": [
        "from ale_py.roms import Breakout\n",
        "env = EnvALE(Breakout, feature_type='Basic')\n",
        "print(env.action_space)\n",
        "alpha = 0.5\n",
        "lam = 0.9\n",
        "#we can upload previous weights as as tensor, or initialize at 0\n",
        "previous_weights = 0\n",
        "\n",
        "\n",
        "evaluator = Sarsa(alpha, lam, replacing_trace, 28672, previous_weights)\n",
        "agent = BaseAgent()\n",
        "env.reset(do_record=False)\n",
        "weights = agent.PHI_EB(evaluator, env, beta=0.05, t_end=10000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<Action.NOOP: 0>, <Action.FIRE: 1>, <Action.RIGHT: 3>, <Action.LEFT: 4>]\n",
            "starting iterations\n",
            "rhos: tensor([1., 1., 1.,  ..., 1., 1., 1.])\n",
            "Iteration #0\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rho_t 0.0\n",
            "min rho_i_t: tensor(0.7500)\n",
            "new_rho_t tensor(2.8026e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(0.)\n",
            "estimation: tensor(0.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([50.,  0.,  0.,  ...,  0.,  0.,  0.])\n",
            "Iteration #1\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.7500, 0.7500, 0.7500,  ..., 0.7500, 0.7500, 0.7500])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.5000)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(11500.)\n",
            "estimation: tensor(11500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([100.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #2\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.8333, 0.8333, 0.8333,  ..., 0.8333, 0.8333, 0.8333])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3750)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(23000.)\n",
            "estimation: tensor(23000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([150.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #3\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.8750, 0.8750, 0.8750,  ..., 0.8750, 0.8750, 0.8750])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3000)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(34500.)\n",
            "estimation: tensor(34500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([200.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #4\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9000, 0.9000, 0.9000,  ..., 0.9000, 0.9000, 0.9000])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2500)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(46000.)\n",
            "estimation: tensor(46000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([250.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #5\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9167, 0.9167, 0.9167,  ..., 0.9167, 0.9167, 0.9167])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2143)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(57500.)\n",
            "estimation: tensor(57500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([300.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #6\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9286, 0.9286, 0.9286,  ..., 0.9286, 0.9286, 0.9286])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1875)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(69000.)\n",
            "estimation: tensor(69000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([350.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #7\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9375, 0.9375, 0.9375,  ..., 0.9375, 0.9375, 0.9375])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1667)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(80500.)\n",
            "estimation: tensor(80500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([400.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #8\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9444, 0.9444, 0.9444,  ..., 0.9444, 0.9444, 0.9444])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1500)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(92000.)\n",
            "estimation: tensor(92000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([450.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #9\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9500, 0.9500, 0.9500,  ..., 0.9500, 0.9500, 0.9500])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1364)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(103500.)\n",
            "estimation: tensor(103500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([500.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #10\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9545, 0.9545, 0.9545,  ..., 0.9545, 0.9545, 0.9545])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1250)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(115000.)\n",
            "estimation: tensor(115000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([550.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #11\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9583, 0.9583, 0.9583,  ..., 0.9583, 0.9583, 0.9583])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1154)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(126500.)\n",
            "estimation: tensor(126500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([600.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #12\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9615, 0.9615, 0.9615,  ..., 0.9615, 0.9615, 0.9615])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1071)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(138000.)\n",
            "estimation: tensor(138000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([650.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #13\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9643, 0.9643, 0.9643,  ..., 0.9643, 0.9643, 0.9643])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1000)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(149500.)\n",
            "estimation: tensor(149500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([700.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #14\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9667, 0.9667, 0.9667,  ..., 0.9667, 0.9667, 0.9667])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9062)\n",
            "new_rho_t tensor(7.0065e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(161000.)\n",
            "estimation: tensor(161000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([750.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #15\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9688, 0.9688, 0.9688,  ..., 0.9688, 0.9688, 0.9688])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9118)\n",
            "new_rho_t tensor(7.0065e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(172500.)\n",
            "estimation: tensor(172500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([800.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #16\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9706, 0.9706, 0.9706,  ..., 0.9706, 0.9706, 0.9706])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9167)\n",
            "new_rho_t tensor(8.4078e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(184000.)\n",
            "estimation: tensor(184000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([850.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #17\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9722, 0.9722, 0.9722,  ..., 0.9722, 0.9722, 0.9722])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9211)\n",
            "new_rho_t tensor(8.4078e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(195500.)\n",
            "estimation: tensor(195500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([900.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #18\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9737, 0.9737, 0.9737,  ..., 0.9737, 0.9737, 0.9737])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9250)\n",
            "new_rho_t tensor(8.4078e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(207000.)\n",
            "estimation: tensor(207000.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([950.,   0.,   0.,  ...,   0.,   0.,   0.])\n",
            "Iteration #19\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9750, 0.9750, 0.9750,  ..., 0.9750, 0.9750, 0.9750])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9286)\n",
            "new_rho_t tensor(8.4078e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(218500.)\n",
            "estimation: tensor(218500.)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([1000.,    0.,    0.,  ...,    0.,    0.,    0.])\n",
            "Iteration #20\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9762, 0.9762, 0.9762,  ..., 0.9762, 0.9762, 0.9762])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1136)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(231630.0781)\n",
            "estimation: tensor(230000.)\n",
            "delta: tensor(2630.0781)\n",
            "weights: tensor([1131.5039,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #21\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9773, 0.9773, 0.9773,  ..., 0.9773, 0.9773, 0.9773])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1087)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(261916.8281)\n",
            "estimation: tensor(261934.6875)\n",
            "delta: tensor(982.1250)\n",
            "weights: tensor([1180.6101,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #22\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9783, 0.9783, 0.9783,  ..., 0.9783, 0.9783, 0.9783])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1042)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(273215.8125)\n",
            "estimation: tensor(273233.1875)\n",
            "delta: tensor(982.6250)\n",
            "weights: tensor([1229.7413,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #23\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9792, 0.9792, 0.9792,  ..., 0.9792, 0.9792, 0.9792])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1000)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(284941.2500)\n",
            "estimation: tensor(284537.9375)\n",
            "delta: tensor(1403.3125)\n",
            "weights: tensor([1299.9070,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #24\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9800, 0.9800, 0.9800,  ..., 0.9800, 0.9800, 0.9800])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0962)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(300675.5312)\n",
            "estimation: tensor(301118.5312)\n",
            "delta: tensor(557.)\n",
            "weights: tensor([1327.7570,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #25\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9808, 0.9808, 0.9808,  ..., 0.9808, 0.9808, 0.9808])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0926)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(307078.0625)\n",
            "estimation: tensor(307093.4062)\n",
            "delta: tensor(984.6562)\n",
            "weights: tensor([1376.9897,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #26\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9815, 0.9815, 0.9815,  ..., 0.9815, 0.9815, 0.9815])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0893)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(318409.)\n",
            "estimation: tensor(318423.5625)\n",
            "delta: tensor(985.4375)\n",
            "weights: tensor([1426.2616,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #27\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9821, 0.9821, 0.9821,  ..., 0.9821, 0.9821, 0.9821])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0862)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(330177.3438)\n",
            "estimation: tensor(329763.5312)\n",
            "delta: tensor(1413.8125)\n",
            "weights: tensor([1496.9523,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #28\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9828, 0.9828, 0.9828,  ..., 0.9828, 0.9828, 0.9828])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0833)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(346028.9062)\n",
            "estimation: tensor(346475.6250)\n",
            "delta: tensor(553.2812)\n",
            "weights: tensor([1524.6163,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #29\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9833, 0.9833, 0.9833,  ..., 0.9833, 0.9833, 0.9833])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0806)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(352392.5938)\n",
            "estimation: tensor(352403.9688)\n",
            "delta: tensor(988.6250)\n",
            "weights: tensor([1574.0476,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #30\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9839, 0.9839, 0.9839,  ..., 0.9839, 0.9839, 0.9839])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0781)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(363773.8438)\n",
            "estimation: tensor(363783.8438)\n",
            "delta: tensor(990.)\n",
            "weights: tensor([1623.5476,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #31\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9844, 0.9844, 0.9844,  ..., 0.9844, 0.9844, 0.9844])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0758)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(375172.3750)\n",
            "estimation: tensor(375180.9375)\n",
            "delta: tensor(991.4375)\n",
            "weights: tensor([1673.1195,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #32\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9848, 0.9848, 0.9848,  ..., 0.9848, 0.9848, 0.9848])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0735)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(386589.1250)\n",
            "estimation: tensor(386596.0312)\n",
            "delta: tensor(993.0938)\n",
            "weights: tensor([1722.7742,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #33\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9853, 0.9853, 0.9853,  ..., 0.9853, 0.9853, 0.9853])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0714)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(397132.5000)\n",
            "estimation: tensor(398031.8750)\n",
            "delta: tensor(100.6250)\n",
            "weights: tensor([1727.8054,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #34\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9857, 0.9857, 0.9857,  ..., 0.9857, 0.9857, 0.9857])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9306)\n",
            "new_rho_t tensor(9.8091e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(397395.3125)\n",
            "estimation: tensor(398290.8125)\n",
            "delta: tensor(104.5000)\n",
            "weights: tensor([1733.0304,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #35\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9861, 0.9861, 0.9861,  ..., 0.9861, 0.9861, 0.9861])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9324)\n",
            "new_rho_t tensor(9.8091e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(398596.9688)\n",
            "estimation: tensor(398596.9688)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([1783.0304,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #36\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9865, 0.9865, 0.9865,  ..., 0.9865, 0.9865, 0.9865])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9342)\n",
            "new_rho_t tensor(9.8091e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(410096.9688)\n",
            "estimation: tensor(410096.9688)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([1833.0304,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #37\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9868, 0.9868, 0.9868,  ..., 0.9868, 0.9868, 0.9868])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9359)\n",
            "new_rho_t tensor(9.8091e-45)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(421596.9688)\n",
            "estimation: tensor(421596.9688)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([1883.0304,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #38\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9872, 0.9872, 0.9872,  ..., 0.9872, 0.9872, 0.9872])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.9375)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(433096.9688)\n",
            "estimation: tensor(433096.9688)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([1933.0304,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #39\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9875, 0.9875, 0.9875,  ..., 0.9875, 0.9875, 0.9875])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0366)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(440730.9062)\n",
            "estimation: tensor(444596.9688)\n",
            "delta: tensor(-2866.0625)\n",
            "weights: tensor([1789.7273,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #40\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9878, 0.9878, 0.9878,  ..., 0.9878, 0.9878, 0.9878])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0595)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(408057.8750)\n",
            "estimation: tensor(408057.8750)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([1839.7273,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #41\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9881, 0.9881, 0.9881,  ..., 0.9881, 0.9881, 0.9881])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0814)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(419457.8750)\n",
            "estimation: tensor(419457.8750)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([1889.7273,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #42\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9884, 0.9884, 0.9884,  ..., 0.9884, 0.9884, 0.9884])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1023)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(430857.8750)\n",
            "estimation: tensor(430857.8750)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([1939.7273,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #43\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9886, 0.9886, 0.9886,  ..., 0.9886, 0.9886, 0.9886])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0333)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(444648.2188)\n",
            "estimation: tensor(442257.8750)\n",
            "delta: tensor(3390.3438)\n",
            "weights: tensor([2109.2444,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #44\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9889, 0.9889, 0.9889,  ..., 0.9889, 0.9889, 0.9889])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0326)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(483318.3125)\n",
            "estimation: tensor(483338.2188)\n",
            "delta: tensor(980.0938)\n",
            "weights: tensor([2158.2490,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #45\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9891, 0.9891, 0.9891,  ..., 0.9891, 0.9891, 0.9891])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0745)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(495282.3438)\n",
            "estimation: tensor(494502.9688)\n",
            "delta: tensor(1779.3750)\n",
            "weights: tensor([2247.2178,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #46\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9894, 0.9894, 0.9894,  ..., 0.9894, 0.9894, 0.9894])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0729)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(515482.2500)\n",
            "estimation: tensor(515595.3125)\n",
            "delta: tensor(886.9375)\n",
            "weights: tensor([2291.5647,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #47\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9896, 0.9896, 0.9896,  ..., 0.9896, 0.9896, 0.9896])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0714)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(525653.3125)\n",
            "estimation: tensor(525607.3750)\n",
            "delta: tensor(1045.9375)\n",
            "weights: tensor([2343.8616,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #48\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9898, 0.9898, 0.9898,  ..., 0.9898, 0.9898, 0.9898])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0700)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(537560.6875)\n",
            "estimation: tensor(537593.5000)\n",
            "delta: tensor(967.1875)\n",
            "weights: tensor([2392.2209,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #49\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9900, 0.9900, 0.9900,  ..., 0.9900, 0.9900, 0.9900])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0686)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(548566.6250)\n",
            "estimation: tensor(548601.7500)\n",
            "delta: tensor(964.8750)\n",
            "weights: tensor([2440.4646,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #50\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9902, 0.9902, 0.9902,  ..., 0.9902, 0.9902, 0.9902])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0673)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(559457.1250)\n",
            "estimation: tensor(559581.3750)\n",
            "delta: tensor(875.7500)\n",
            "weights: tensor([2484.2522,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #51\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9904, 0.9904, 0.9904,  ..., 0.9904, 0.9904, 0.9904])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0660)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(569490.7500)\n",
            "estimation: tensor(569454.5625)\n",
            "delta: tensor(1036.1875)\n",
            "weights: tensor([2536.0615,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #52\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9906, 0.9906, 0.9906,  ..., 0.9906, 0.9906, 0.9906])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0278)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(580511.1875)\n",
            "estimation: tensor(581319.6250)\n",
            "delta: tensor(191.5625)\n",
            "weights: tensor([2545.6396,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #53\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9907, 0.9907, 0.9907,  ..., 0.9907, 0.9907, 0.9907])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0273)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(582659.1250)\n",
            "estimation: tensor(582697.3125)\n",
            "delta: tensor(961.8125)\n",
            "weights: tensor([2593.7302,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #54\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9909, 0.9909, 0.9909,  ..., 0.9909, 0.9909, 0.9909])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0625)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(594334.6875)\n",
            "estimation: tensor(593635.2500)\n",
            "delta: tensor(1699.4375)\n",
            "weights: tensor([2678.7021,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #55\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9911, 0.9911, 0.9911,  ..., 0.9911, 0.9911, 0.9911])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0614)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(612942.8125)\n",
            "estimation: tensor(613735.1250)\n",
            "delta: tensor(207.6875)\n",
            "weights: tensor([2689.0864,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #56\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9912, 0.9912, 0.9912,  ..., 0.9912, 0.9912, 0.9912])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0603)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(614636.2500)\n",
            "estimation: tensor(615312.8750)\n",
            "delta: tensor(323.3750)\n",
            "weights: tensor([2705.2551,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #57\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9914, 0.9914, 0.9914,  ..., 0.9914, 0.9914, 0.9914])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3305)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(616798.1250)\n",
            "estimation: tensor(618325.3125)\n",
            "delta: tensor(-527.1875)\n",
            "weights: tensor([2678.8958,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #58\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9915, 0.9915, 0.9915,  ..., 0.9915, 0.9915, 0.9915])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3417)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(610788.1875)\n",
            "estimation: tensor(610788.1875)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([2728.8958,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #59\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9917, 0.9917, 0.9917,  ..., 0.9917, 0.9917, 0.9917])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3525)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(622188.1875)\n",
            "estimation: tensor(622188.1875)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([2778.8958,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #60\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9918, 0.9918, 0.9918,  ..., 0.9918, 0.9918, 0.9918])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3629)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(633588.2500)\n",
            "estimation: tensor(633588.2500)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([2828.8958,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #61\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9919, 0.9919, 0.9919,  ..., 0.9919, 0.9919, 0.9919])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3730)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(644988.2500)\n",
            "estimation: tensor(644988.2500)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([2878.8958,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #62\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9921, 0.9921, 0.9921,  ..., 0.9921, 0.9921, 0.9921])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0234)\n",
            "new_rho_t tensor(1.2612e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(656388.2500)\n",
            "estimation: tensor(656388.2500)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([2928.8958,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #63\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9922, 0.9922, 0.9922,  ..., 0.9922, 0.9922, 0.9922])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0231)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(671393.9375)\n",
            "estimation: tensor(667788.2500)\n",
            "delta: tensor(4605.6875)\n",
            "weights: tensor([3159.1802,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #64\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9923, 0.9923, 0.9923,  ..., 0.9923, 0.9923, 0.9923])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0379)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(724445.3125)\n",
            "estimation: tensor(724436.3750)\n",
            "delta: tensor(1008.9375)\n",
            "weights: tensor([3209.6270,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #65\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9924, 0.9924, 0.9924,  ..., 0.9924, 0.9924, 0.9924])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0224)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(735890.3125)\n",
            "estimation: tensor(736065.5000)\n",
            "delta: tensor(824.8125)\n",
            "weights: tensor([3250.8677,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #66\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9925, 0.9925, 0.9925,  ..., 0.9925, 0.9925, 0.9925])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0221)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(745303.5000)\n",
            "estimation: tensor(745389.4375)\n",
            "delta: tensor(914.0625)\n",
            "weights: tensor([3296.5708,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #67\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9926, 0.9926, 0.9926,  ..., 0.9926, 0.9926, 0.9926])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0217)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(755839.3125)\n",
            "estimation: tensor(755830.4375)\n",
            "delta: tensor(1008.8750)\n",
            "weights: tensor([3347.0146,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #68\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9928, 0.9928, 0.9928,  ..., 0.9928, 0.9928, 0.9928])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0214)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(767418.)\n",
            "estimation: tensor(767458.5625)\n",
            "delta: tensor(959.4375)\n",
            "weights: tensor([3394.9866,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #69\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9929, 0.9929, 0.9929,  ..., 0.9929, 0.9929, 0.9929])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0211)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(778424.2500)\n",
            "estimation: tensor(778467.5625)\n",
            "delta: tensor(956.6875)\n",
            "weights: tensor([3442.8210,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #70\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9930, 0.9930, 0.9930,  ..., 0.9930, 0.9930, 0.9930])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0208)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(789344.7500)\n",
            "estimation: tensor(789442.1250)\n",
            "delta: tensor(902.6250)\n",
            "weights: tensor([3487.9524,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #71\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9931, 0.9931, 0.9931,  ..., 0.9931, 0.9931, 0.9931])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0205)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(799737.7500)\n",
            "estimation: tensor(799740.1250)\n",
            "delta: tensor(997.6250)\n",
            "weights: tensor([3537.8337,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #72\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9932, 0.9932, 0.9932,  ..., 0.9932, 0.9932, 0.9932])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0203)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(811292.7500)\n",
            "estimation: tensor(811227.1875)\n",
            "delta: tensor(1065.5625)\n",
            "weights: tensor([3591.1118,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #73\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9932, 0.9932, 0.9932,  ..., 0.9932, 0.9932, 0.9932])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0333)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(823552.8125)\n",
            "estimation: tensor(823564.5000)\n",
            "delta: tensor(988.3125)\n",
            "weights: tensor([3640.5273,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #74\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9933, 0.9933, 0.9933,  ..., 0.9933, 0.9933, 0.9933])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0197)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(834737.8125)\n",
            "estimation: tensor(834934.8125)\n",
            "delta: tensor(803.)\n",
            "weights: tensor([3680.6772,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #75\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9934, 0.9934, 0.9934,  ..., 0.9934, 0.9934, 0.9934])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0195)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(843684.1250)\n",
            "estimation: tensor(843985.7500)\n",
            "delta: tensor(698.3750)\n",
            "weights: tensor([3715.5959,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #76\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9935, 0.9935, 0.9935,  ..., 0.9935, 0.9935, 0.9935])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.1987)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(848729.3125)\n",
            "estimation: tensor(851723.4375)\n",
            "delta: tensor(-1994.1250)\n",
            "weights: tensor([3615.8896,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #77\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9936, 0.9936, 0.9936,  ..., 0.9936, 0.9936, 0.9936])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0190)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(825796.9375)\n",
            "estimation: tensor(825797.)\n",
            "delta: tensor(999.9375)\n",
            "weights: tensor([3665.8865,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #78\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9937, 0.9937, 0.9937,  ..., 0.9937, 0.9937, 0.9937])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2188)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(837296.1250)\n",
            "estimation: tensor(837296.1250)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([3715.8865,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #79\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9937, 0.9937, 0.9937,  ..., 0.9937, 0.9937, 0.9937])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2284)\n",
            "new_rho_t tensor(1.1210e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(848796.1250)\n",
            "estimation: tensor(848796.1250)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([3765.8865,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #80\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9938, 0.9938, 0.9938,  ..., 0.9938, 0.9938, 0.9938])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2378)\n",
            "new_rho_t tensor(1.2612e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(860296.1250)\n",
            "estimation: tensor(860296.1250)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([3815.8865,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #81\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9939, 0.9939, 0.9939,  ..., 0.9939, 0.9939, 0.9939])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2470)\n",
            "new_rho_t tensor(1.2612e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(871796.1250)\n",
            "estimation: tensor(871796.1250)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([3865.8865,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #82\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9940, 0.9940, 0.9940,  ..., 0.9940, 0.9940, 0.9940])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2560)\n",
            "new_rho_t tensor(1.2612e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(887835.3750)\n",
            "estimation: tensor(883296.1250)\n",
            "delta: tensor(5539.2500)\n",
            "weights: tensor([4142.8491,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #83\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9940, 0.9940, 0.9940,  ..., 0.9940, 0.9940, 0.9940])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2647)\n",
            "new_rho_t tensor(1.2612e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(951540.8125)\n",
            "estimation: tensor(951540.8125)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([4192.8491,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #84\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9941, 0.9941, 0.9941,  ..., 0.9941, 0.9941, 0.9941])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2733)\n",
            "new_rho_t tensor(1.2612e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(963140.8125)\n",
            "estimation: tensor(963140.8125)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([4242.8491,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #85\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9942, 0.9942, 0.9942,  ..., 0.9942, 0.9942, 0.9942])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.2816)\n",
            "new_rho_t tensor(1.2612e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(974740.8125)\n",
            "estimation: tensor(974740.8125)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([4292.8491,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #86\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9943, 0.9943, 0.9943,  ..., 0.9943, 0.9943, 0.9943])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0284)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(990427.6875)\n",
            "estimation: tensor(986340.8125)\n",
            "delta: tensor(5086.8750)\n",
            "weights: tensor([4547.1929,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #87\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9943, 0.9943, 0.9943,  ..., 0.9943, 0.9943, 0.9943])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0393)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1050279.8750)\n",
            "estimation: tensor(1049497.7500)\n",
            "delta: tensor(1782.1250)\n",
            "weights: tensor([4636.2993,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #88\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9944, 0.9944, 0.9944,  ..., 0.9944, 0.9944, 0.9944])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0278)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1071236.2500)\n",
            "estimation: tensor(1070973.7500)\n",
            "delta: tensor(1262.5000)\n",
            "weights: tensor([4699.4243,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #89\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9944, 0.9944, 0.9944,  ..., 0.9944, 0.9944, 0.9944])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0275)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1085831.7500)\n",
            "estimation: tensor(1085901.1250)\n",
            "delta: tensor(930.6250)\n",
            "weights: tensor([4745.9556,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #90\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9945, 0.9945, 0.9945,  ..., 0.9945, 0.9945, 0.9945])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0272)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1097461.3750)\n",
            "estimation: tensor(1096641.6250)\n",
            "delta: tensor(1819.7500)\n",
            "weights: tensor([4836.9429,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #91\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9946, 0.9946, 0.9946,  ..., 0.9946, 0.9946, 0.9946])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0269)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1117757.)\n",
            "estimation: tensor(1118599.3750)\n",
            "delta: tensor(157.6250)\n",
            "weights: tensor([4844.8242,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #92\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9946, 0.9946, 0.9946,  ..., 0.9946, 0.9946, 0.9946])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0266)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1119555.2500)\n",
            "estimation: tensor(1119588.)\n",
            "delta: tensor(967.2500)\n",
            "weights: tensor([4893.1865,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #93\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9947, 0.9947, 0.9947,  ..., 0.9947, 0.9947, 0.9947])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0368)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1131073.7500)\n",
            "estimation: tensor(1130790.8750)\n",
            "delta: tensor(1282.8750)\n",
            "weights: tensor([4957.3301,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #94\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9947, 0.9947, 0.9947,  ..., 0.9947, 0.9947, 0.9947])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0260)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1145622.)\n",
            "estimation: tensor(1145970.2500)\n",
            "delta: tensor(651.7500)\n",
            "weights: tensor([4989.9175,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #95\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9948, 0.9948, 0.9948,  ..., 0.9948, 0.9948, 0.9948])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0258)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1153250.8750)\n",
            "estimation: tensor(1153192.6250)\n",
            "delta: tensor(1058.2500)\n",
            "weights: tensor([5042.8301,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #96\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9948, 0.9948, 0.9948,  ..., 0.9948, 0.9948, 0.9948])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0255)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1165143.7500)\n",
            "estimation: tensor(1165543.2500)\n",
            "delta: tensor(600.5000)\n",
            "weights: tensor([5072.8550,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #97\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.LEFT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9949, 0.9949, 0.9949,  ..., 0.9949, 0.9949, 0.9949])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0253)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1172363.3750)\n",
            "estimation: tensor(1172116.8750)\n",
            "delta: tensor(1246.5000)\n",
            "weights: tensor([5135.1802,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #98\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9949, 0.9949, 0.9949,  ..., 0.9949, 0.9949, 0.9949])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0250)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1187774.5000)\n",
            "estimation: tensor(1186842.6250)\n",
            "delta: tensor(1931.8750)\n",
            "weights: tensor([5231.7739,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #99\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.NOOP\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9950, 0.9950, 0.9950,  ..., 0.9950, 0.9950, 0.9950])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.0446)\n",
            "new_rho_t 1e-323\n",
            "Nhat_t:  tensor(nan)\n",
            "reward:  1000.0\n",
            "state value: tensor(1205399.2500)\n",
            "estimation: tensor(1210215.3750)\n",
            "delta: tensor(-3816.1250)\n",
            "weights: tensor([5040.9678,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #100\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9950, 0.9950, 0.9950,  ..., 0.9950, 0.9950, 0.9950])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3873)\n",
            "new_rho_t tensor(1.4013e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(1159904.3750)\n",
            "estimation: tensor(1161130.7500)\n",
            "delta: tensor(-226.3750)\n",
            "weights: tensor([5029.6489,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #101\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9951, 0.9951, 0.9951,  ..., 0.9951, 0.9951, 0.9951])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3932)\n",
            "new_rho_t tensor(1.5414e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(1157278.3750)\n",
            "estimation: tensor(1157278.3750)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([5079.6489,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #102\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.RIGHT\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9951, 0.9951, 0.9951,  ..., 0.9951, 0.9951, 0.9951])\n",
            "rho_t tensor(0.)\n",
            "min rho_i_t: tensor(0.3990)\n",
            "new_rho_t tensor(1.5414e-44)\n",
            "Nhat_t:  tensor(0.)\n",
            "reward:  1000.0\n",
            "state value: tensor(1168878.2500)\n",
            "estimation: tensor(1168878.2500)\n",
            "delta: tensor(1000.)\n",
            "weights: tensor([5129.6489,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
            "           0.0000])\n",
            "Iteration #103\n",
            "--------------------------------------------------------------\n",
            "took action:  Action.FIRE\n",
            "phi: tensor([ True, False, False,  ..., False, False, False])\n",
            "rhos: tensor([0.9952, 0.9952, 0.9952,  ..., 0.9952, 0.9952, 0.9952])\n",
            "rho_t tensor(0.)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-82383f1b9ced>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBaseAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_record\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPHI_EB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-b318029abfa7>\u001b[0m in \u001b[0;36mPHI_EB\u001b[0;34m(self, evaluator, env, beta, t_end)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateRho_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'min rho_i_t: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrhos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;31m#compute rho_t+1(phi)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    646\u001b[0m                           \u001b[0;34m'iterations executed (and might lead to errors or silently give '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m                           'incorrect results).', category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(weights)\n",
        "df.to_csv('weights.csv',index=False)"
      ],
      "metadata": {
        "id": "APocVw8BnLu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.unique(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQtDBe1sO05G",
        "outputId": "4503ed62-f61d-445f-f88f-6487afa2cb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.2507e+14, -2.0636e+14, -1.5559e+14, -1.4064e+14, -1.2728e+14,\n",
              "        -1.2308e+14, -1.1768e+14, -8.7606e+13, -7.5274e+13, -7.5155e+13,\n",
              "        -6.6346e+13, -6.0275e+13, -5.5657e+13, -4.2305e+13, -3.6145e+13,\n",
              "        -2.4283e+13, -2.2198e+13,  0.0000e+00,  4.0416e+13,  5.3655e+13,\n",
              "         6.4767e+13,  7.6218e+13,  8.0885e+13,  8.4170e+13,  8.9790e+13,\n",
              "         9.7415e+13,  9.8442e+13,  9.9528e+13,  1.0906e+14,  1.1365e+14,\n",
              "         1.1610e+14,  1.1933e+14,  1.1967e+14,  1.2303e+14,  1.2381e+14,\n",
              "         1.2587e+14,  1.2837e+14,  1.2884e+14,  1.3777e+14,  1.3794e+14,\n",
              "         1.4520e+14,  1.5968e+14,  1.7645e+14,  1.7650e+14,  1.7855e+14,\n",
              "         1.8481e+14,  1.8539e+14,  1.9363e+14,  2.0072e+14,  2.0472e+14,\n",
              "         2.1115e+14,  2.1236e+14,  2.1405e+14,  2.2247e+14,  2.3898e+14,\n",
              "         2.4331e+14,  2.4374e+14,  2.4441e+14,  2.5321e+14,  2.5674e+14,\n",
              "         2.6116e+14,  2.6192e+14,  2.6217e+14,  2.6347e+14,  2.7327e+14,\n",
              "         2.9621e+14,  3.0445e+14,  3.0828e+14,  3.1369e+14,  3.1480e+14,\n",
              "         3.2439e+14,  3.2709e+14,  3.2900e+14,  3.3475e+14,  3.3695e+14,\n",
              "         3.4372e+14,  3.4454e+14,  3.5155e+14,  3.5257e+14,  3.5371e+14,\n",
              "         3.5736e+14,  3.6064e+14,  3.7282e+14,  3.7393e+14,  3.7622e+14,\n",
              "         3.7640e+14,  3.7840e+14,  3.8497e+14,  3.9684e+14,  4.0227e+14,\n",
              "         4.0266e+14,  4.1210e+14,  4.2070e+14,  4.6221e+14,  4.7587e+14,\n",
              "         5.1219e+14,  5.4709e+14,  5.6398e+14,  6.0167e+14,  6.2366e+14,\n",
              "         6.5704e+14,  6.6481e+14,  6.7354e+14,  6.7693e+14,  5.5723e+15,\n",
              "         5.7787e+15,  6.6621e+15])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##optimisation tests\n",
        "# %%timeit\n",
        "# t = 1\n",
        "# M = 28000\n",
        "# counts = np.zeros(M)\n",
        "# rhos = np.ones(M)\n",
        "# for i in range(M): #M is the number of features of phi\n",
        "#   counts[i] += 1 #since we add phi to the seen states, all the counts are increased by one for t+1\n",
        "#   rhos[i] = (counts[i]+0.5)/(t+1)"
      ],
      "metadata": {
        "id": "7cKj8q7PjAT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #best one\n",
        "# %%timeit\n",
        "# t = 1\n",
        "# M = 28000\n",
        "# counts = np.zeros(M)\n",
        "# rhos = np.ones(M)\n",
        "# rhos = (counts+1.5)/(t+1)"
      ],
      "metadata": {
        "id": "wphENKYrjSD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #compute rho_t(phi) (feature visit-density)\n",
        "# %%timeit\n",
        "# t = 0\n",
        "# t_end = 30\n",
        "# M = 10000\n",
        "# phis = np.zeros((t_end,M))\n",
        "# counts = np.zeros(M)\n",
        "# states = np.zeros((t_end,M))\n",
        "# rhos = np.ones(M)\n",
        "# while t < t_end:\n",
        "#   phi = phis[t]\n",
        "#   if t > 0:\n",
        "#     rho_t = 1\n",
        "#     for i in range(M):\n",
        "#       counts[i] = 0\n",
        "#       for step in range(t):\n",
        "#         if phi[i] == states[step,i]:\n",
        "#           counts[i] += 1\n",
        "#       rhos[i] = (counts[i]+0.5)/(t+1)\n",
        "#       rho_t = rho_t*rhos[i]\n",
        "#   else:\n",
        "#     rho_t = 0.5**M\n",
        "\n",
        "#   #updating rho (other method)\n",
        "#   states[t] = phi\n",
        "#   counts += 1\n",
        "#   rhos = (counts+0.5)/(t+2)\n",
        "  \n",
        "#   t += 1"
      ],
      "metadata": {
        "id": "sqH7kGYDk2Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #compute rho_t(phi) (feature visit-density)\n",
        "# %%timeit\n",
        "# t = 0\n",
        "# t_end = 5000\n",
        "# M = 1000\n",
        "# phis = np.zeros((t_end,M))\n",
        "# counts = np.zeros(M)\n",
        "# states = np.zeros((t_end,M))\n",
        "# rhos = np.ones(M)\n",
        "# while t < t_end:\n",
        "#   phi = phis[t]\n",
        "#   if t > 0:\n",
        "#     rho_t = 1\n",
        "#     counts = np.zeros(M)\n",
        "#     for step in range(t):\n",
        "#       counts[np.where(phi == states[step])] += 1\n",
        "#     rhos = (counts+0.5)/(t+1)\n",
        "#     rho_t = np.prod(rhos)\n",
        "#   else:\n",
        "#     rho_t = 0.5**M\n",
        "\n",
        "#   #updating rho (other method)\n",
        "#   states[t] = phi\n",
        "#   counts += 1\n",
        "#   rhos = (counts+0.5)/(t+2)\n",
        "  \n",
        "#   t += 1"
      ],
      "metadata": {
        "id": "j82o_Tzqoi2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #version 3 of optimisation\n",
        "# %%timeit\n",
        "# t = 0\n",
        "# t_end = 5000\n",
        "# M = 1000\n",
        "# #phis = [[0,1,0],[0,1,0],[0,1,0],[1,1,0]]\n",
        "# phis = np.zeros((t_end,M))\n",
        "# counts = np.zeros(M)\n",
        "# states = np.zeros((t_end,M))\n",
        "# rhos = np.ones(M)\n",
        "# while t < t_end:\n",
        "#   phi = phis[t]\n",
        "#   if t > 0:\n",
        "#     rho_t = 1\n",
        "#     counts = np.sum(np.where(phi == states[0:t],1,0),axis=0)\n",
        "#     rhos = (counts+0.5)/(t+1)\n",
        "#     rho_t = np.prod(rhos)\n",
        "#   else:\n",
        "#     rho_t = 0.5**M\n",
        "\n",
        "#   #updating rho (other method)\n",
        "#   states[t] = phi\n",
        "#   counts += 1\n",
        "#   rhos = (counts+0.5)/(t+2)\n",
        "  \n",
        "#   t += 1"
      ],
      "metadata": {
        "id": "1irC6C3no0gV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}