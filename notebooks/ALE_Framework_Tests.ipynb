{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALE Framework Tests",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tinynja/Sarsa-phi-EB/blob/main/notebooks/ALE_Framework_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z3aqLItDvig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb29bda-2bc4-47a1-c76f-75331cd2d84b"
      },
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !rm -rf *\n",
        "    !git clone https://github.com/Tinynja/Sarsa-phi-EB\n",
        "    !mv Sarsa-phi-EB/* .\n",
        "    !rm -rf Sarsa-phi-EB\n",
        "    # DON'T install packages defined in Pipfile_Colab_exclude\n",
        "    !sed -ri \"/$(tr '\\n' '|' < Pipfile_Colab_exclude)/d\" Pipfile\n",
        "else:\n",
        "    print('Skipping GitHub cloning since not running in Colab.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sarsa-phi-EB'...\n",
            "remote: Enumerating objects: 354, done.\u001b[K\n",
            "remote: Counting objects: 100% (354/354), done.\u001b[K\n",
            "remote: Compressing objects: 100% (300/300), done.\u001b[K\n",
            "remote: Total 354 (delta 121), reused 209 (delta 41), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (354/354), 854.88 KiB | 1.48 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdficWqDDviq",
        "outputId": "7847954e-24c9-4c10-8b79-dae8b33b6107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install required dependencies\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Some dependencies required for displaying episode\n",
        "    !apt install -y python-opengl xvfb 1> /dev/null\n",
        "    !pip install pyvirtualdisplay 1> /dev/null\n",
        "    # Colab doesn't support pipenv, hence we convert Pipfile into requirements.txt\n",
        "    if 'requirements_Colab.txt' not in os.listdir():\n",
        "        !pip install pipenv\n",
        "        !pipenv lock -r > requirements.txt\n",
        "    !pip install -r requirements_Colab.txt 1> /dev/null\n",
        "else:\n",
        "    !pipenv install 1> /dev/null"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_YFCigY7Dvis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a74866c-7197-48d1-d707-6de72d1bafff",
        "collapsed": true
      },
      "source": [
        "# Import all supported ROMs into ALE\n",
        "!ale-import-roms ROMS"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m[SUPPORTED]    \u001b[0m               enduro         ROMS/Enduro (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      elevator_action ROMS/Elevator Action (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix ROMS/Asterix (AKA Taz) (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       name_this_game ROMS/Name This Game (Guardians of Treasure) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           backgammon ROMS/Backgammon (Paddle) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          private_eye    ROMS/Private Eye (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         demon_attack ROMS/Demon Attack (Death from Above) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             carnival       ROMS/Carnival (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             seaquest       ROMS/Seaquest (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong    ROMS/Donkey Kong (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              frogger        ROMS/Frogger (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math ROMS/Basic Math - Math (Math Pack) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             air_raid ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         yars_revenge ROMS/Yars' Revenge (Time Freeze) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          double_dunk ROMS/Double Dunk (Super Basketball) (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               zaxxon         ROMS/Zaxxon (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               gopher ROMS/Gopher (Gopher Attack) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert         ROMS/Q-bert (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             trondead ROMS/TRON - Deadly Discs (TRON Joystick) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround ROMS/Surround - Chase (Blockade) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            king_kong      ROMS/King Kong (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               boxing ROMS/Boxing - La Boxe (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          laser_gates ROMS/Laser Gates (AKA Innerspace) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            space_war ROMS/Space War - Space Star (32 in 1) (1988).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             gravitar       ROMS/Gravitar (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                krull          ROMS/Krull (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               amidar         ROMS/Amidar (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               casino ROMS/Casino - Poker Plus (Paddle) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 pong ROMS/Video Olympics - Pong Sports (Paddle) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             defender       ROMS/Defender (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         lost_luggage ROMS/Lost Luggage (Airport Mayhem) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           videochess ROMS/Video Chess (Computer Chess) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong    ROMS/Donkey Kong (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               skiing ROMS/Skiing - Le Ski (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            tutankham      ROMS/Tutankham (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              othello        ROMS/Othello (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              pitfall ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           earthworld ROMS/SwordQuest - EarthWorld (Adventure I, SwordQuest I - EarthWorld) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            asteroids      ROMS/Asteroids (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            atlantis2    ROMS/Atlantis II (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       journey_escape ROMS/Journey Escape (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          star_gunner     ROMS/Stargunner (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             breakout ROMS/Breakout - Breakaway IV (Paddle) (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert        ROMS/Q. Bert (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         flag_capture ROMS/Flag Capture - Capture (Capture the Flag) (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix ROMS/Asterix (AKA Taz) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          word_zapper ROMS/Word Zapper (Word Grabber) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       miniature_golf ROMS/Miniature Golf - Arcade Golf (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              solaris ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         sir_lancelot   ROMS/Sir Lancelot (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      keystone_kapers ROMS/Keystone Kapers - Raueber und Gendarm (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball ROMS/Video Pinball - Arcade Pinball (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              hangman ROMS/Hangman - Spelling (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             robotank ROMS/Robot Tank (Robotank) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              assault ROMS/Assault (AKA Sky Alien) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             superman       ROMS/Superman (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             entombed ROMS/Entombed (Maze Chase, Pharaoh's Tomb, Zombie) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math ROMS/Fun with Numbers (AKA Basic Math) (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pacman        ROMS/Pac-Man (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              phoenix        ROMS/Phoenix (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              berzerk        ROMS/Berzerk (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              koolaid ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 hero       ROMS/H.E.R.O. (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         darkchambers ROMS/Dark Chambers (Dungeon, Dungeon Masters) (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       kung_fu_master ROMS/Kung-Fu Master (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pooyan         ROMS/Pooyan (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               tennis ROMS/Tennis - Le Tennis (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           beam_rider      ROMS/Beamrider (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 klax           ROMS/Klax (1991).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        haunted_house ROMS/Haunted House (Mystery Mansion, Graves' Manor, Nightmare Manor) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             galaxian       ROMS/Galaxian (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m    montezuma_revenge ROMS/Montezuma's Revenge - Featuring Panama Joe (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              venture        ROMS/Venture (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          battle_zone     ROMS/Battlezone (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          road_runner    ROMS/Road Runner (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      chopper_command ROMS/Chopper Command (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              turmoil        ROMS/Turmoil (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            blackjack ROMS/Blackjack - Black Jack (Gambling) (Paddle) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           ice_hockey ROMS/Ice Hockey - Le Hockey Sur Glace (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            ms_pacman    ROMS/Ms. Pac-Man (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball ROMS/Pinball (AKA Video Pinball) (Zellers).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            adventure      ROMS/Adventure (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               kaboom ROMS/Kaboom! (Paddle) (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            up_n_down     ROMS/Up 'n Down (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           bank_heist ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            frostbite      ROMS/Frostbite (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                mr_do        ROMS/Mr. Do! (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       video_checkers ROMS/Video Checkers - Checkers - Atari Video Checkers (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             pitfall2 ROMS/Pitfall II - Lost Caverns (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              bowling        ROMS/Bowling (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       space_invaders ROMS/Space Invaders (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           time_pilot     ROMS/Time Pilot (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             kangaroo       ROMS/Kangaroo (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert         ROMS/Q-bert (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m     human_cannonball ROMS/Human Cannonball - Cannon Man (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             atlantis ROMS/Atlantis (Lost City of Atlantis) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             crossbow       ROMS/Crossbow (1988).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       tic_tac_toe_3d ROMS/3-D Tic-Tac-Toe (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                   et ROMS/E.T. - The Extra-Terrestrial (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            videocube ROMS/Atari Video Cube (Atari Cube, Video Cube) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        wizard_of_wor  ROMS/Wizard of Wor (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        fishing_derby  ROMS/Fishing Derby (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            centipede      ROMS/Centipede (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        crazy_climber  ROMS/Crazy Climber (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              freeway        ROMS/Freeway (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           mario_bros    ROMS/Mario Bros. (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                alien          ROMS/Alien (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            jamesbond ROMS/James Bond 007 (James Bond Agent 007) (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            riverraid     ROMS/River Raid (1982).bin\n",
            "\n",
            "\n",
            "\n",
            "Imported 110 / 110 ROMs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SfKRFx8Dvit",
        "outputId": "e3adb6b2-aabd-4fc5-b333-3db25515750e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#### ALE-related imports ####\n",
        "\n",
        "# Built-in libraries\n",
        "import re\n",
        "import sys\n",
        "import timeit\n",
        "import base64\n",
        "import pickle\n",
        "import logging\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Pypi libraries\n",
        "import gym\n",
        "import torch\n",
        "# import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ale_py import ALEInterface, SDL_SUPPORT\n",
        "import ale_py.roms as ROMS\n",
        "\n",
        "# Episode display\n",
        "from PIL import Image\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "# Configuration\n",
        "CUDA = 'cuda' if torch.cuda.device_count() else 'cpu'\n",
        "CPU = 'cpu'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for external in metadata.entry_points().get(self.group, []):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvRecorder:\n",
        "    def __init__(self, env, out_dir='./results'):\n",
        "        self.out_dir = Path(out_dir)\n",
        "        self.out_dir.mkdir(exist_ok=True)\n",
        "        self.out_dir = self.out_dir.resolve()\n",
        "        self.env = env\n",
        "\n",
        "        # Workspace variables\n",
        "        self.__frame_count_padding = 0\n",
        "        self.__timestep = 0\n",
        "    \n",
        "    def __call__(self, choose_action=None, max_steps=-1, height=300):\n",
        "        self.record_episode_and_show(choose_action=choose_action, max_steps=max_steps, height=height)\n",
        "    \n",
        "    def record_episode_and_show(self, choose_action=None, max_steps=-1, height=300):\n",
        "        self.record_episode(choose_action=choose_action, max_steps=max_steps)\n",
        "        self.show_recording(height=height)\n",
        "    \n",
        "    def record_episode(self, choose_action=None, max_steps=-1):\n",
        "        \"\"\"Generate an episode and record it as record.mp4\n",
        "        Args:\n",
        "        choose_action -- callable method that returns the next action based on the current observation\n",
        "        max_steps     -- maximum number of steps after which force end episode\n",
        "        \"\"\"\n",
        "        choose_action = choose_action or (lambda _: self.env.action_space.sample())\n",
        "\n",
        "        self._clear_recording()\n",
        "        self.__timestep, done, observation =  0, False, self.env.reset()\n",
        "        self._record_frame()\n",
        "        while not done and self.__timestep != max_steps:\n",
        "            print(f'\\rRecording episode, timestep {self.__timestep+1}...', end='')\n",
        "            action = choose_action(observation)\n",
        "            observation, _, done, _ = self.env.step(action)\n",
        "            self.__timestep += 1\n",
        "            self._record_frame()\n",
        "\n",
        "        if self.__timestep == max_steps and not done:\n",
        "            print('\\nWarning: `max_steps` reached before episode terminated')\n",
        "        else:\n",
        "            print()\n",
        "\n",
        "        self._export_as_mp4()\n",
        "\n",
        "    def show_recording(self, height=300):\n",
        "        \"\"\"Show a .mp4 video in html format of the recorded episode\"\"\"\n",
        "        filepath = self.out_dir.joinpath('record.mp4')\n",
        "        video_b64 = base64.b64encode(filepath.read_bytes())\n",
        "        html = f'''<video alt=\"{filepath}\" autoplay loop controls style=\"height:{height}px\">\n",
        "                        <source src=\"data:video/mp4;base64,{video_b64.decode('ascii')}\" type=\"video/mp4\" />\n",
        "                   </video>'''\n",
        "        ipythondisplay.display(ipythondisplay.HTML(data=html))\n",
        "\n",
        "    def _clear_recording(self):\n",
        "        # This is a new episode, delete previously recorded steps\n",
        "        self.out_dir.joinpath('record').mkdir(exist_ok=True)\n",
        "        for step_png in self.out_dir.glob('record/step_*.png'):\n",
        "            step_png.unlink()\n",
        "        if self.out_dir.joinpath('record.mp4').exists():\n",
        "            self.out_dir.joinpath('record.mp4').unlink()\n",
        "    \n",
        "    def _record_frame(self):\n",
        "        # Record current timestep png\n",
        "        img = Image.fromarray(env.render('rgb_array'))\n",
        "        out_path = self.out_dir.joinpath(f'record/step_{self.__timestep}.png')\n",
        "        img.save(str(out_path))\n",
        "\n",
        "    def _export_as_mp4(self):\n",
        "        \"\"\"Convert the recorded set of png files into an mp4 video\"\"\"\n",
        "        self._standardize_frame_count_padding()\n",
        "        in_dir = self.out_dir.joinpath('record')\n",
        "        in_pattern = f'step_%0{self.__frame_count_padding}d.png'\n",
        "        out_file = self.out_dir.joinpath('record.mp4')\n",
        "        !cd $in_dir; ffmpeg -hide_banner -loglevel error -r 60 -i $in_pattern -vcodec libx264 -crf 25 -pix_fmt yuv420p -y $out_file\n",
        "    \n",
        "    def _standardize_frame_count_padding(self):\n",
        "        self.__frame_count_padding = len(str(self.__timestep))\n",
        "        number_pattern = re.compile('\\d+')\n",
        "        png_abs_glob = 'step_*.png'\n",
        "        for png_path in self.out_dir.joinpath('record').glob(png_abs_glob):\n",
        "            ts = int(number_pattern.search(png_path.stem).group(0))\n",
        "            new_name = png_path.parent.joinpath(f'step_{ts:0{self.__frame_count_padding}d}.png')\n",
        "            png_path.rename(new_name)"
      ],
      "metadata": {
        "id": "ZFF_G0wToLuO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAs6_UzuKGjz"
      },
      "source": [
        "class features:\n",
        "    @staticmethod\n",
        "    def basic(frame, palette, background, crop_size=torch.Tensor([15,10]), flatten=True):\n",
        "        # For each color in palette, tell if each pixel is that color (e.g. 210,160,128)\n",
        "        colors_in_pixels = ((frame-background).unsqueeze(-2) == palette).all(-1)\n",
        "        # Split the image into n tiles, each with dimension `crop_size` (e.g. 14,16,15,10,128)\n",
        "        cropped_colors_in_pixels = torch.stack(torch.stack(colors_in_pixels.split(crop_size[1],dim=-2)).split(crop_size[0],dim=-3))\n",
        "        # Apply logical or inside each cropped image (e.g. 14,16,128)\n",
        "        cropped_features = cropped_colors_in_pixels.any(3).any(2)\n",
        "        # Flatten the features (e.g. 28672)\n",
        "        return cropped_features.flatten() if flatten else cropped_features\n",
        "    \n",
        "    @staticmethod\n",
        "    def b_pros(frame, palette, background, crop_size=torch.Tensor([15,10])):\n",
        "        raise NotImplementedError()\n",
        "        basic_features = features.basic(frame, palette, background, crop_size=crop_size)\n",
        "        b_pros_features = torch.combinations(basic_features)\n",
        "        return b_pros_features\n",
        "    \n",
        "    @staticmethod\n",
        "    def discretized_float(number, low, high, n_intervals):\n",
        "        feature = torch.zeros(n_intervals+2, dtype=torch.bool)\n",
        "        if number < low:\n",
        "            feature[0] = True\n",
        "        elif number >= high:\n",
        "            feature[-1] = True\n",
        "        else:\n",
        "            interval = (high-low)/n_intervals\n",
        "            index = int((number-low)/interval)\n",
        "            feature[index+1] = True\n",
        "        return feature"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryEnv:\n",
        "    def __init__(self, env_name, n_features, device=CUDA, *args, **kwargs):\n",
        "        self.env = gym.make(env_name, *args, **kwargs)\n",
        "        self.n_features = int(n_features/self.env.observation_space.shape[0])\n",
        "        self.device = device\n",
        "\n",
        "        self.action_space = self.env.action_space\n",
        "        self.observation_space = self.env.observation_space\n",
        "    \n",
        "    def reset(self, *args, **kwargs):\n",
        "        observation = self.env.reset(*args, **kwargs)\n",
        "        return self._binarize_observation(observation)\n",
        "\n",
        "    def step(self, *args, **kwargs):\n",
        "        observation, reward, done, info = self.env.step(*args, **kwargs)\n",
        "        observation = self._binarize_observation(observation)\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        return self.env.render(*args, **kwargs)\n",
        "    \n",
        "    def _binarize_observation(self, observation):\n",
        "        bin_features = []\n",
        "        lows, highs = self.env.observation_space.low, self.env.observation_space.high\n",
        "        for i,obs in enumerate(observation):\n",
        "            bin_features.append(features.discretized_float(obs, lows[i], highs[i], n_intervals=self.n_features))\n",
        "        return torch.cat(bin_features).to(self.device)"
      ],
      "metadata": {
        "id": "hmZZlTI8gDia"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0fNQ7lIlEdl"
      },
      "source": [
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "class EnvALE:\n",
        "    def __init__(self, rom, out_dir='results', seed=0, feature_type='ScreenRGB',\n",
        "                 regen_bg=False, bg_samples=18000, device=CUDA):\n",
        "        self.rom = rom\n",
        "        self.rom_name = rom.stem\n",
        "        self.feature_type = feature_type\n",
        "        self.device = device\n",
        "\n",
        "        self.out_dir = Path(out_dir)\n",
        "        self.out_dir.mkdir(exist_ok=True)\n",
        "        self.out_dir = self.out_dir.resolve()\n",
        "\n",
        "        # ALE\n",
        "        self.ale = ALEInterface()\n",
        "        self.ale.setInt(\"random_seed\", seed)\n",
        "        self.ale.loadROM(rom)\n",
        "\n",
        "        # gym action_space compatibility\n",
        "        action_set = self.ale.getMinimalActionSet()\n",
        "        self.action_space = Discrete(len(action_set))\n",
        "        self.action_space.action_set = action_set\n",
        "\n",
        "        # color palette\n",
        "        self.color_palette = self._get_color_palette().to(self.device)\n",
        "\n",
        "        self._bg_path = Path(f'./backgrounds/{self.rom_name}.pickle')\n",
        "        if regen_bg or not self._bg_path.exists() or not self._bg_path.is_file():\n",
        "            self.background = self._get_background(n_samples=bg_samples)\n",
        "        else:\n",
        "            with open(self._bg_path, 'rb') as file:\n",
        "                self.background = pickle.load(file).to(self.device)\n",
        "        \n",
        "        self._set_observe_method(feature_type)\n",
        "        self.observation_space = Box(low=0, high=1, shape=self._observe().shape, dtype=bool)\n",
        "\n",
        "    def reset(self, do_record=False):\n",
        "        self.ale.reset_game()\n",
        "        observation = self._observe()\n",
        "        \n",
        "        return observation\n",
        "        \n",
        "    def step(self, action):\n",
        "        if isinstance(action, int):\n",
        "            action = self.action_space.action_set[action]\n",
        "\n",
        "        reward = self.ale.act(action)\n",
        "        observation = self._observe()\n",
        "        done = self.ale.game_over()\n",
        "                \n",
        "        return observation, reward, done, None\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        if mode == 'rgb_array':\n",
        "            return self.ale.getScreenRGB()\n",
        "        else:\n",
        "            raise ValueError(f'render mode `{mode}` is not supported')\n",
        "\n",
        "    def _set_observe_method(self, feature_type):\n",
        "        if feature_type == 'ScreenRGB':\n",
        "            self._observe = lambda: torch.from_numpy(self.ale.getScreenRGB()).to(self.device)\n",
        "        elif feature_type == 'ScreenGrayscale':\n",
        "            self._observe = lambda: torch.from_numpy(self.ale.getScreenGrayscale()).to(self.device)\n",
        "        elif feature_type == 'Basic':\n",
        "            self._observe = lambda: features.basic(frame=torch.from_numpy(self.ale.getScreenRGB()).to(self.device),\n",
        "                                                   palette=self.color_palette,\n",
        "                                                   background=self.background)\n",
        "        elif feature_type == 'B-PROS':\n",
        "            self._observe = lambda: features.b_pros(frame=torch.from_numpy(self.ale.getScreenRGB()).to(self.device),\n",
        "                                                    palette=self.color_palette,\n",
        "                                                    background=self.background)\n",
        "        else:\n",
        "            raise NotImplementedError(f'Feature type `{feature_type}` is not supported')\n",
        "        \n",
        "    def _observe(self):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def _get_color_palette(self):\n",
        "        result = subprocess.run(['python', '-c', f'__import__(\"ale_py\").ALEInterface().loadROM(\"{str(self.rom)}\")'], capture_output=True)\n",
        "        palette_name = result.stderr.decode().splitlines()[6].strip().split()[-1]\n",
        "        with open(f'palettes/{palette_name}_Palette.pickle', 'rb') as file:\n",
        "            palette = pickle.load(file)\n",
        "        return palette\n",
        "    \n",
        "    def _get_background(self, n_samples):\n",
        "        bg_feature_type = 'ScreenRGB' if self.feature_type not in ['ScreenGrayscale',] else 'ScreenGrayscale'\n",
        "        self._set_observe_method(bg_feature_type)\n",
        "        \n",
        "        sample_i = 0\n",
        "        pixel_histogram = torch.zeros((*self.ale.getScreenDims(), self.color_palette.shape[0]), dtype=torch.int32).to(self.device)\n",
        "        while sample_i < n_samples:\n",
        "            done, observation = False, self.reset()\n",
        "            while not done and sample_i < n_samples:\n",
        "                if not sample_i%10:\n",
        "                    print(f'\\rGenerating background... {sample_i}/{n_samples} samples ({sample_i/n_samples:.0%})', end='')\n",
        "                action = self.action_space.sample()\n",
        "                observation, reward, done, info = self.step(action)\n",
        "                colors_in_pixels = (observation.unsqueeze(-2) == self.color_palette).all(-1)\n",
        "                pixel_histogram += colors_in_pixels\n",
        "                sample_i += 1\n",
        "        print('\\r', end='')\n",
        "        background_ids = pixel_histogram.argmax(axis=-1)\n",
        "        background = self.color_palette[background_ids]\n",
        "        \n",
        "        self._bg_path.parent.mkdir(exist_ok=True)\n",
        "        with open(self._bg_path, 'wb') as file:\n",
        "            pickle.dump(background.cpu(), file)\n",
        "        \n",
        "        return background"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "if print(\"Skipped background regeneration.\"):\n",
        "# if True:\n",
        "\n",
        "    from ale_py.roms import *\n",
        "    games_to_generate_bg = [Breakout, MontezumaRevenge, Venture, Qbert, Frostbite, Freeway]\n",
        "\n",
        "    for game in games_to_generate_bg:\n",
        "        print(f'\\n{game.stem}')\n",
        "        env = EnvALE(game, regen_bg=True, bg_samples=100)\n",
        "        bg_np = env.background.cpu().to(torch.uint8).numpy()\n",
        "        display(Image.fromarray(bg_np))"
      ],
      "metadata": {
        "id": "-jjm2aZ9oMGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bff9e34-b608-41bc-9125-6988dcb02096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped background regeneration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "if print(\"Skipped displaying stored backgrounds to reduce ouptuts.\"):\n",
        "# if True:\n",
        "\n",
        "    for filepath in Path('backgrounds').iterdir():\n",
        "        print(f'\\nBackground in `{filepath.resolve()}`')\n",
        "        with open(filepath, 'rb') as file:\n",
        "            bg = pickle.load(file)\n",
        "        display(Image.fromarray(bg.to(torch.uint8).numpy()))"
      ],
      "metadata": {
        "id": "DT3TM-u1ISRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61c27c7-2dde-42c8-f1a1-d55efbc076e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped displaying stored backgrounds to reduce ouptuts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh8WdcHh4Z8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51904bb4-8446-445d-e46d-7ad83fd7e939"
      },
      "source": [
        "#@title\n",
        "\n",
        "if print(\"Skipped manual test.\"):\n",
        "# if True:\n",
        "\n",
        "    env = EnvALE(ROMS.Breakout)\n",
        "    recorder = EnvRecorder(env)\n",
        "    recorder()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped manual test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SarsaPhiEBAgent:\n",
        "    def __init__(self, env, alpha=0.5, gamma=0.99, lam=0.9, beta=0.05, init_action=1, step_repeat_count=1, device=CUDA, debug=False):\n",
        "        \"\"\"An agent using Sarsa(lambda) algorithm with:\n",
        "            - Linear Function Approximation (SGD)\n",
        "            - Replacing Traces\n",
        "            - Exploration-Bonus\n",
        "        \n",
        "        Args:\n",
        "                      env -- gym-like environment\n",
        "                    alpha -- step size\n",
        "                    gamma -- discount factor\n",
        "                      lam -- trace decay\n",
        "                     beta -- exploration bonus parameter\n",
        "              init_action -- action to take after an environment reset\n",
        "        step_repeat_count -- numer of times to repeat an action every timestep (under-the-hood) \n",
        "                   device -- device on which to store tensors\n",
        "                    debug -- enable debug output\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.beta = beta\n",
        "        self.init_action = init_action\n",
        "        self.step_repeat_count = step_repeat_count\n",
        "        self.device = device\n",
        "        self.epsilon = 0.1 #hardcoded\n",
        "\n",
        "        self.__log = logging.Logger(name=self.__class__.__name__,\n",
        "                                    level=(logging.DEBUG if debug else logging.WARNING))\n",
        "\n",
        "        # Workspace variables\n",
        "        self.feature_space_shape = (self.env.action_space.n, *self.env.reset().shape)\n",
        "        self.weights = torch.zeros(self.feature_space_shape).to(self.device)\n",
        "        self._counts = torch.zeros(self.feature_space_shape).to(self.device)\n",
        "        self._total_steps = 0\n",
        "\n",
        "    def learn(self, n_steps):\n",
        "        start_time, done = timeit.default_timer(), True\n",
        "        for i in range(n_steps):\n",
        "            # Information display\n",
        "            avg_time = (timeit.default_timer()-start_time)/(i or 1)\n",
        "            print(f'\\rSarsaPhiEB learning iteration {self._total_steps+1}/{self._total_steps-i+n_steps} ({1000*avg_time:.2f}ms/iter, remaining {(n_steps-i)*avg_time:.0f}s)    ', end='')\n",
        "\n",
        "            # Reset env when episode ends\n",
        "            if done:\n",
        "                next_phi = self.env.reset()\n",
        "                if self.init_action is not None:\n",
        "                    next_phi, _, _, _ = self.env.step(self.init_action)\n",
        "                next_action = self._choose_action(next_phi)\n",
        "                traces = torch.zeros(self.feature_space_shape).to(self.device)\n",
        "\n",
        "            # Advance to next timestep\n",
        "            phi, action = next_phi, next_action\n",
        "            active_features = phi.nonzero()\n",
        "            self._total_steps += 1\n",
        "\n",
        "            # Take an action\n",
        "            next_phi, reward, done, _ = self._step_repeat(action)\n",
        "            next_action = self._choose_action(next_phi)\n",
        "\n",
        "            # Apply exploration bonus\n",
        "            self._counts[action,active_features] += 1\n",
        "            reward += self._calc_exploration_bonus(phi, action)\n",
        "\n",
        "            # RL Algorithm : Sarsa(lambda) LFA(SGD) Replacing Traces\n",
        "            traces*= self.gamma*self.lam\n",
        "            traces[action,active_features] = phi[active_features].to(traces.dtype)\n",
        "            if not done:\n",
        "                delta = reward + self.gamma*self._action_value(next_phi, next_action) - self._action_value(phi, action)\n",
        "            else:\n",
        "                # In terminal state, all state-action values are 0\n",
        "                delta = reward + 0 - self._action_value(phi, action)\n",
        "            self.weights += self.alpha * delta * traces\n",
        "\n",
        "        print(f'\\nTotal elapsed time: {datetime.utcfromtimestamp(timeit.default_timer()-start_time).strftime(\"%H:%M:%S.%f\")}')\n",
        "    \n",
        "    def _step_repeat(self, action):\n",
        "        for i in range(self.step_repeat_count):\n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "            if done: break\n",
        "        return observation, reward, done, info\n",
        "    \n",
        "    def _action_value(self, phi, action):\n",
        "        return (self.weights[action]@phi.to(self.weights.dtype)).item()\n",
        "    \n",
        "    def _choose_action(self, phi):\n",
        "        if torch.rand(1) > self.epsilon:\n",
        "            return (self.weights@phi.to(self.weights.dtype)).argmax().item()\n",
        "        else:\n",
        "            return self.env.action_space.sample()\n",
        "    \n",
        "    def _calc_exploration_bonus(self, phi, action):\n",
        "        # Compute the exploration bonus\n",
        "        phi_occ = torch.cat((self._counts[action, phi], self._total_steps-self._counts[action, ~phi])).to(self.device)\n",
        "        rho = ((phi_occ+1/2) / (self._total_steps+1)).prod()\n",
        "        rho_prime = ((phi_occ+1+1/2) / (self._total_steps+1+1)).prod()\n",
        "        pseudocount = (rho*(1-rho_prime)) / (rho_prime-rho)\n",
        "        exploration_bonus = self.beta/pseudocount.sqrt()\n",
        "        return exploration_bonus"
      ],
      "metadata": {
        "id": "5kQC-tTUHgFl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = BinaryEnv('CartPole-v1', 10000)\n",
        "#env = EnvALE(ROMS.Breakout, feature_type='Basic')\n",
        "\n",
        "agent = SarsaPhiEBAgent(env, init_action=0)\n",
        "recorder = EnvRecorder(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyjGd3iWHV6H",
        "outputId": "961cbaa8-f634-46e6-c110-b2f6ef39993e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in float_scalars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.learn(30000)"
      ],
      "metadata": {
        "id": "CxQakEP7ocBo",
        "outputId": "ea29d05f-1643-4ec3-dacf-2393e4728934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rSarsaPhiEB learning iteration 1/30000 (0.00ms/iter, remaining 0s)    \rSarsaPhiEB learning iteration 2/30000 (7.78ms/iter, remaining 233s)    \rSarsaPhiEB learning iteration 3/30000 (6.19ms/iter, remaining 186s)    \rSarsaPhiEB learning iteration 4/30000 (5.24ms/iter, remaining 157s)    \rSarsaPhiEB learning iteration 5/30000 (4.48ms/iter, remaining 134s)    \rSarsaPhiEB learning iteration 6/30000 (4.02ms/iter, remaining 121s)    \rSarsaPhiEB learning iteration 7/30000 (3.78ms/iter, remaining 113s)    \rSarsaPhiEB learning iteration 8/30000 (3.57ms/iter, remaining 107s)    \rSarsaPhiEB learning iteration 9/30000 (3.38ms/iter, remaining 101s)    \rSarsaPhiEB learning iteration 10/30000 (3.35ms/iter, remaining 101s)    \rSarsaPhiEB learning iteration 11/30000 (3.26ms/iter, remaining 98s)    \rSarsaPhiEB learning iteration 12/30000 (3.16ms/iter, remaining 95s)    \rSarsaPhiEB learning iteration 13/30000 (3.09ms/iter, remaining 93s)    \rSarsaPhiEB learning iteration 14/30000 (3.06ms/iter, remaining 92s)    \rSarsaPhiEB learning iteration 15/30000 (2.99ms/iter, remaining 90s)    \rSarsaPhiEB learning iteration 16/30000 (2.93ms/iter, remaining 88s)    \rSarsaPhiEB learning iteration 17/30000 (2.91ms/iter, remaining 87s)    \rSarsaPhiEB learning iteration 18/30000 (2.88ms/iter, remaining 86s)    \rSarsaPhiEB learning iteration 19/30000 (2.84ms/iter, remaining 85s)    \rSarsaPhiEB learning iteration 20/30000 (2.86ms/iter, remaining 86s)    \rSarsaPhiEB learning iteration 21/30000 (2.83ms/iter, remaining 85s)    \rSarsaPhiEB learning iteration 22/30000 (2.80ms/iter, remaining 84s)    \rSarsaPhiEB learning iteration 23/30000 (2.79ms/iter, remaining 84s)    \rSarsaPhiEB learning iteration 24/30000 (2.76ms/iter, remaining 83s)    \rSarsaPhiEB learning iteration 25/30000 (2.73ms/iter, remaining 82s)    \rSarsaPhiEB learning iteration 26/30000 (2.72ms/iter, remaining 81s)    \rSarsaPhiEB learning iteration 27/30000 (2.70ms/iter, remaining 81s)    \rSarsaPhiEB learning iteration 28/30000 (2.68ms/iter, remaining 80s)    \rSarsaPhiEB learning iteration 29/30000 (2.70ms/iter, remaining 81s)    \rSarsaPhiEB learning iteration 30/30000 (2.68ms/iter, remaining 80s)    \rSarsaPhiEB learning iteration 31/30000 (2.66ms/iter, remaining 80s)    \rSarsaPhiEB learning iteration 32/30000 (2.64ms/iter, remaining 79s)    \rSarsaPhiEB learning iteration 33/30000 (2.63ms/iter, remaining 79s)    \rSarsaPhiEB learning iteration 34/30000 (2.61ms/iter, remaining 78s)    \rSarsaPhiEB learning iteration 35/30000 (2.60ms/iter, remaining 78s)    \rSarsaPhiEB learning iteration 36/30000 (2.58ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 37/30000 (2.57ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 38/30000 (2.58ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 39/30000 (2.57ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 40/30000 (2.56ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 41/30000 (2.55ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 42/30000 (2.54ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 43/30000 (2.53ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 44/30000 (2.52ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 45/30000 (2.51ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 46/30000 (2.51ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 47/30000 (2.49ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 48/30000 (2.48ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 49/30000 (2.50ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 50/30000 (2.49ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 51/30000 (2.48ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 52/30000 (2.47ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 53/30000 (2.55ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 54/30000 (2.56ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 55/30000 (2.57ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 56/30000 (2.56ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 57/30000 (2.57ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 58/30000 (2.56ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 59/30000 (2.56ms/iter, remaining 77s)    \rSarsaPhiEB learning iteration 60/30000 (2.55ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 61/30000 (2.54ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 62/30000 (2.53ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 63/30000 (2.53ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 64/30000 (2.52ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 65/30000 (2.52ms/iter, remaining 76s)    \rSarsaPhiEB learning iteration 66/30000 (2.52ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 67/30000 (2.52ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 68/30000 (2.51ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 69/30000 (2.51ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 70/30000 (2.50ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 71/30000 (2.50ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 72/30000 (2.50ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 73/30000 (2.49ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 74/30000 (2.50ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 75/30000 (2.49ms/iter, remaining 75s)    \rSarsaPhiEB learning iteration 76/30000 (2.49ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 77/30000 (2.48ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 78/30000 (2.48ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 79/30000 (2.47ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 80/30000 (2.46ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 81/30000 (2.46ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 82/30000 (2.46ms/iter, remaining 74s)    \rSarsaPhiEB learning iteration 83/30000 (2.46ms/iter, remaining 74s)    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in float_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SarsaPhiEB learning iteration 30000/30000 (2.44ms/iter, remaining 0s)    \n",
            "Total elapsed time: 00:01:13.327527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recorder(choose_action=agent._choose_action, max_steps=1000)"
      ],
      "metadata": {
        "id": "91iMb12MmrSG",
        "outputId": "080cdb31-0f26-4dc0-b9a9-e2b91cf53150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in float_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rRecording episode, timestep 1...\rRecording episode, timestep 2...\rRecording episode, timestep 3...\rRecording episode, timestep 4...\rRecording episode, timestep 5...\rRecording episode, timestep 6...\rRecording episode, timestep 7...\rRecording episode, timestep 8...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"/content/results/record.mp4\" autoplay loop controls style=\"height:300px\">\n",
              "                        <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "                   </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2017-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib\n",
        "# matplotlib.use('Agg')\n",
        "# import matplotlib.pyplot as plt\n",
        "# from math import floor\n",
        "# from tqdm import tqdm\n",
        "\n",
        "\n",
        "# all possible actions\n",
        "ACTIONS = range(4)\n",
        "\n",
        "# discount is always 1.0 in these experiments\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "# use optimistic initial value, so it's ok to set epsilon to 0\n",
        "EPSILON = 0.01\n",
        "\n",
        "# maximum steps per episode\n",
        "STEP_LIMIT = 5000\n",
        "\n",
        "\n",
        "# get action at @position and @velocity based on epsilon greedy policy and @valueFunction  #########################    use our own get_action. modified it, may work as intended\n",
        "def get_action(observation, valueFunction):\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "        return np.random.choice(ACTIONS)\n",
        "    values = []\n",
        "    for action in ACTIONS:\n",
        "        values.append(valueFunction.value(observation))  \n",
        "    maxi = np.max(values)\n",
        "    bestactions = np.where(values==maxi,1.0,0.0)\n",
        "    for i in range(len(ACTIONS)):\n",
        "      bestactions[i] = bestactions[i]*np.random.uniform()\n",
        "    action = np.argmax(bestactions)\n",
        "    return action\n",
        "\n",
        "\n",
        "\n",
        "# replacing trace update rule\n",
        "# @trace: old trace (will be modified)\n",
        "# @activeTiles: current active tile indices\n",
        "# @lam: lambda\n",
        "# @return: new trace for convenience\n",
        "def replacing_trace(trace, activeTiles, lam):\n",
        "    active = (torch.arange(len(trace)).to(device)[None,...] == activeTiles.flatten()[...,None]).any(0)\n",
        "    trace[active] = 1\n",
        "    trace[~active] *= lam * DISCOUNT\n",
        "    return trace\n",
        "\n",
        "\n",
        "\n",
        "# wrapper class for Sarsa(lambda)\n",
        "class Sarsa:\n",
        "    # In this example I use the tiling software instead of implementing standard tiling by myself\n",
        "    # One important thing is that tiling is only a map from (state, action) to a series of indices\n",
        "    # It doesn't matter whether the indices have meaning, only if this map satisfy some property\n",
        "    # View the following webpage for more information\n",
        "    # http://incompleteideas.net/sutton/tiles/tiles3.html\n",
        "    # @maxSize: the maximum # of indices\n",
        "    #the hashing is a lfa?\n",
        "    def __init__(self, step_size, lam, trace_update=replacing_trace, max_size=28672, initial_weights=0):\n",
        "        self.max_size = max_size\n",
        "        self.trace_update = trace_update\n",
        "        self.lam = lam\n",
        "\n",
        "        # divide step size equally to each tiling\n",
        "        self.step_size = step_size/10\n",
        "\n",
        "        # weight for each tile\n",
        "        if initial_weights == 0:\n",
        "          self.weights =torch.zeros(max_size).to(device) #max size is the number of features?\n",
        "        else:\n",
        "          self.weights = initial_weights.to(device)\n",
        "\n",
        "        # trace for each tile\n",
        "        self.trace = torch.zeros(max_size).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # estimate the value of given state and action\n",
        "    def value(self, observation):\n",
        "        active_tiles = np.nonzero(observation)\n",
        "        return self.weights[active_tiles].sum()\n",
        "\n",
        "    # learn with given state, action and target\n",
        "    def learn(self, observation, target):\n",
        "        active_tiles = np.nonzero(observation)\n",
        "        estimation = self.weights[active_tiles].sum()\n",
        "        delta = target - estimation\n",
        "        #print('estimation array: ' + str(self.weights[active_tiles]))\n",
        "        # print('estimation: ' + str(self.weights[active_tiles].sum()))\n",
        "        if self.trace_update == replacing_trace:\n",
        "            self.trace_update(self.trace, active_tiles, self.lam)\n",
        "        else:\n",
        "            raise Exception('Unexpected Trace Type')\n",
        "        self.weights += self.step_size * delta * self.trace\n",
        "        # print('delta: ' + str(delta))\n",
        "        # print('weights: ' +  str(self.weights))\n",
        "\n",
        "\n",
        "# play Mountain Car for one episode based on given method `evaluator`\n",
        "# return: total steps in this episode\n",
        "def play(evaluator, env):\n",
        "\n",
        "    action = random.choice(ACTIONS)\n",
        "    steps = 0\n",
        "    while True:\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        next_action = get_action(next_observation, evaluator)    #########################    use our own get_action  ??? modified it, may work as intented\n",
        "        steps += 1\n",
        "        target = reward + DISCOUNT * evaluator.value(next_observation)          ############# use our own value function ??? modified it, may work as intented\n",
        "        evaluator.learn(observation, target)\n",
        "        observation = next_observation\n",
        "        action = next_action\n",
        "        if done:\n",
        "            break\n",
        "        if steps >= STEP_LIMIT:\n",
        "            print('Step Limit Exceeded!')\n",
        "            break\n",
        "    return steps\n",
        "\n",
        "class BaseAgent:\n",
        "  \"\"\" The base agent class function.\n",
        "  \"\"\"\n",
        "  def __init__(self, nb_features=28672):\n",
        "    #nothing for now\n",
        "    self.gamma = 1\n",
        "    self.features = nb_features\n",
        "    self.rhos = torch.ones(self.features).to(device) #stores the rho_i values\n",
        "\n",
        "\n",
        "  def takeAction(self, t):\n",
        "    phis = [[0,1,0],[0,1,0],[0,1,0],[1,0,1]]\n",
        "    return phis[t]\n",
        "\n",
        "\n",
        "  def updateRho_i(self, counts, t):\n",
        "    M = self.features\n",
        "    self.rhos = (counts+1.5)/(t+1)\n",
        "    return 0\n",
        "\n",
        "\n",
        "  def PHI_EB(self, evaluator, env, beta=0.05, t_end=200):\n",
        "    t = 0\n",
        "    M = self.features #number of features\n",
        "    counts = torch.zeros(M).to(device)\n",
        "    states = torch.zeros(t_end,M).to(device) #stores the previous phis for all timesteps\n",
        "\n",
        "    action = 1 #starting the game for the agent on the first game\n",
        "    old_phi = env._observe()\n",
        "    print('starting iterations')\n",
        "    print('rhos: ' + str(self.rhos))\n",
        "    while t < t_end:\n",
        "    #   print(\"Iteration #{}\".format(t))\n",
        "      #observe phi(s), reward\n",
        "    #   phi, reward, done, info = obs.clone(), 0, False, None\n",
        "      phi, reward, done, info = env.step(action)\n",
        "    #   print(phi.shape)\n",
        "    #   print('--------------------------------------------------------------')\n",
        "    #   print('took action: ', env.action_space[action])\n",
        "      next_action = get_action(phi, evaluator)\n",
        "    #   print('phi: ' + str(phi))\n",
        "      \n",
        "      #compute rho_t(phi) (feature visit-density)\n",
        "      if t > 0:\n",
        "        counts = (phi==states[0:t]).sum(0)\n",
        "        # print(counts)\n",
        "        self.rhos = (counts+0.5)/(t+1)\n",
        "        # print('rhos: ' + str(self.rhos))\n",
        "        rho_t = torch.prod(self.rhos)\n",
        "      else:\n",
        "        rho_t = 0.5**M\n",
        "    #   print('rho_t ' + str(rho_t))\n",
        "      #update all rho_i with observed phi\n",
        "      states[t] = phi\n",
        "      self.updateRho_i(counts, t+1)\n",
        "    #   print('min rho_i_t: ' + str(min(self.rhos)))\n",
        "      \n",
        "      #compute rho_t+1(phi)\n",
        "      new_rho_t = 1\n",
        "      # THIS IS A BOTTLENECK (tested in CPU mode: 74ms -> 178ms)\n",
        "      for i in range(M):\n",
        "        new_rho_t = new_rho_t*self.rhos[i]\n",
        "      if new_rho_t <= 1e-323: #this is to avoid division by zero, might need to be tweaked\n",
        "        new_rho_t = 1e-323\n",
        "    #   print('new_rho_t ' + str(new_rho_t))\n",
        "\n",
        "      #compute Nhat_t(s)\n",
        "      Nhat_t = rho_t*(1-new_rho_t)/(new_rho_t-rho_t)\n",
        "    #   print('Nhat_t: ',   Nhat_t)\n",
        "      if Nhat_t <= 1e-323: #this is to avoid division by zero again, might need to be tweaked\n",
        "        Nhat_t = torch.tensor([1e-323]).to(device)\n",
        "\n",
        "      #compute R(s,a) (empirical reward)\n",
        "      explorationBonus = beta/torch.sqrt(Nhat_t)\n",
        "      if torch.isnan(explorationBonus) or explorationBonus >= 1e3:\n",
        "        explorationBonus = 1e3\n",
        "\n",
        "      reward = reward + explorationBonus\n",
        "    #   print('reward: ',reward)\n",
        "\n",
        "    #   print('state value: ' + str(evaluator.value(phi)))\n",
        "      #pass phi(s) and reward to RL algo to update theta_t\n",
        "      target = reward + self.gamma * evaluator.value(phi)          ############# use our own value function ??? modified it, may work as intented\n",
        "      # THIS IS A BOTTLENECK (tested in CPU mode: 190ms -> 207ms)\n",
        "      evaluator.learn(old_phi, target)\n",
        "\n",
        "      if done:\n",
        "        #break\n",
        "        env.reset()\n",
        "        action = 1\n",
        "        old_phi = env._observe()\n",
        "        print('episode ended on step ', t, 'starting a new one')\n",
        "      else:\n",
        "        old_phi = phi\n",
        "        action = next_action\n",
        "      t += 1\n",
        "      continue\n",
        "\n",
        "\n",
        "    return evaluator.weights\n",
        "\n",
        "# from ale_py.roms import Breakout\n",
        "# import timeit\n",
        "# env = EnvALE(Breakout, feature_type='Basic')\n",
        "# print(env.action_space)\n",
        "# alpha = 0.5\n",
        "# lam = 0.9\n",
        "# #we can upload previous weights as as tensor, or initialize at 0\n",
        "# previous_weights = 0\n",
        "# obs = torch.randint(0,2,(28672,), dtype=bool).to(device)\n",
        "# t_end = 10\n",
        "\n",
        "# evaluator = Sarsa(alpha, lam, replacing_trace, 28672, previous_weights)\n",
        "# agent = BaseAgent()\n",
        "# env.reset(do_record=False)\n",
        "# start_time = timeit.default_timer()\n",
        "# weights = agent.PHI_EB(evaluator, env, beta=0.05, t_end=t_end)\n",
        "# print(f'{1000*(timeit.default_timer()-start_time)/t_end}ms per timestep')"
      ],
      "metadata": {
        "id": "6OmpMlK6HrOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}