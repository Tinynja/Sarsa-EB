{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of ALE Framework Tests",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z3aqLItDvig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ffda72-5941-412f-914d-39f22bbef691"
      },
      "source": [
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    !rm -rf *\n",
        "    !git clone https://github.com/Tinynja/Sarsa-phi-EB\n",
        "    !mv Sarsa-phi-EB/* .\n",
        "    !rm -rf Sarsa-phi-EB\n",
        "    # DON'T install packages defined in Pipfile_Colab_exclude\n",
        "    !sed -ri \"/$(tr '\\n' '|' < Pipfile_Colab_exclude)/d\" Pipfile\n",
        "else:\n",
        "    print('Skipping GitHub cloning since not running in Colab.')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sarsa-phi-EB'...\n",
            "remote: Enumerating objects: 358, done.\u001b[K\n",
            "remote: Counting objects: 100% (358/358), done.\u001b[K\n",
            "remote: Compressing objects: 100% (304/304), done.\u001b[K\n",
            "remote: Total 358 (delta 123), reused 209 (delta 41), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (358/358), 860.79 KiB | 11.96 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdficWqDDviq",
        "outputId": "3802f474-68ee-43b2-89c0-0c2cd9ce23a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install required dependencies\n",
        "import os\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    # Some dependencies required for displaying episode\n",
        "    !apt install -y python-opengl xvfb 1> /dev/null\n",
        "    !pip install pyvirtualdisplay 1> /dev/null\n",
        "    # Colab doesn't support pipenv, hence we convert Pipfile into requirements.txt\n",
        "    if 'requirements_Colab.txt' not in os.listdir():\n",
        "        !pip install pipenv\n",
        "        !pipenv lock -r > requirements.txt\n",
        "    !pip install -r requirements_Colab.txt 1> /dev/null\n",
        "else:\n",
        "    !pipenv install 1> /dev/null"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_YFCigY7Dvis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9a1b44-9c8c-4290-c62f-82500579e9ac",
        "collapsed": true
      },
      "source": [
        "# Import all supported ROMs into ALE\n",
        "!ale-import-roms ROMS"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround ROMS/Surround - Chase (Blockade) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            king_kong      ROMS/King Kong (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      elevator_action ROMS/Elevator Action (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             air_raid ROMS/Air Raid (Men-A-Vision) (PAL) ~.bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             crossbow       ROMS/Crossbow (1988).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           backgammon ROMS/Backgammon (Paddle) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              solaris ROMS/Solaris (The Last Starfighter, Star Raiders II, Universe) (1986).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          word_zapper ROMS/Word Zapper (Word Grabber) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            asteroids      ROMS/Asteroids (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix ROMS/Asterix (AKA Taz) (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pacman        ROMS/Pac-Man (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               pooyan         ROMS/Pooyan (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         lost_luggage ROMS/Lost Luggage (Airport Mayhem) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              berzerk        ROMS/Berzerk (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             galaxian       ROMS/Galaxian (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               kaboom ROMS/Kaboom! (Paddle) (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               casino ROMS/Casino - Poker Plus (Paddle) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        haunted_house ROMS/Haunted House (Mystery Mansion, Graves' Manor, Nightmare Manor) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              hangman ROMS/Hangman - Spelling (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               zaxxon         ROMS/Zaxxon (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         darkchambers ROMS/Dark Chambers (Dungeon, Dungeon Masters) (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              koolaid ROMS/Kool-Aid Man (Kool Aid Pitcher Man) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         yars_revenge ROMS/Yars' Revenge (Time Freeze) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             superman       ROMS/Superman (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              phoenix        ROMS/Phoenix (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           ice_hockey ROMS/Ice Hockey - Le Hockey Sur Glace (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            frostbite      ROMS/Frostbite (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       space_invaders ROMS/Space Invaders (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m     human_cannonball ROMS/Human Cannonball - Cannon Man (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      keystone_kapers ROMS/Keystone Kapers - Raueber und Gendarm (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             trondead ROMS/TRON - Deadly Discs (TRON Joystick) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          double_dunk ROMS/Double Dunk (Super Basketball) (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            blackjack ROMS/Blackjack - Black Jack (Gambling) (Paddle) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          battle_zone     ROMS/Battlezone (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          road_runner    ROMS/Road Runner (1989).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong    ROMS/Donkey Kong (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             atlantis ROMS/Atlantis (Lost City of Atlantis) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             carnival       ROMS/Carnival (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             entombed ROMS/Entombed (Maze Chase, Pharaoh's Tomb, Zombie) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          donkey_kong    ROMS/Donkey Kong (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert         ROMS/Q-bert (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           videochess ROMS/Video Chess (Computer Chess) (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m    montezuma_revenge ROMS/Montezuma's Revenge - Featuring Panama Joe (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            riverraid     ROMS/River Raid (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           time_pilot     ROMS/Time Pilot (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            atlantis2    ROMS/Atlantis II (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              assault ROMS/Assault (AKA Sky Alien) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              turmoil        ROMS/Turmoil (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           bank_heist ROMS/Bank Heist (Bonnie & Clyde, Cops 'n' Robbers, Hold-Up, Roaring 20's) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             breakout ROMS/Breakout - Breakaway IV (Paddle) (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             robotank ROMS/Robot Tank (Robotank) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              asterix ROMS/Asterix (AKA Taz) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          star_gunner     ROMS/Stargunner (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       name_this_game ROMS/Name This Game (Guardians of Treasure) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball ROMS/Pinball (AKA Video Pinball) (Zellers).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              frogger        ROMS/Frogger (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              venture        ROMS/Venture (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m      chopper_command ROMS/Chopper Command (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              freeway        ROMS/Freeway (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            jamesbond ROMS/James Bond 007 (James Bond Agent 007) (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             gravitar       ROMS/Gravitar (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           beam_rider      ROMS/Beamrider (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              pitfall ROMS/Pitfall! - Pitfall Harry's Jungle Adventure (Jungle Runner) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         sir_lancelot   ROMS/Sir Lancelot (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       journey_escape ROMS/Journey Escape (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             kangaroo       ROMS/Kangaroo (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 klax           ROMS/Klax (1991).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             seaquest       ROMS/Seaquest (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        video_pinball ROMS/Video Pinball - Arcade Pinball (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               amidar         ROMS/Amidar (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       kung_fu_master ROMS/Kung-Fu Master (1987).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          laser_gates ROMS/Laser Gates (AKA Innerspace) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             pitfall2 ROMS/Pitfall II - Lost Caverns (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         demon_attack ROMS/Demon Attack (Death from Above) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               enduro         ROMS/Enduro (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               boxing ROMS/Boxing - La Boxe (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        wizard_of_wor  ROMS/Wizard of Wor (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m         flag_capture ROMS/Flag Capture - Capture (Capture the Flag) (1978).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            centipede      ROMS/Centipede (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            space_war ROMS/Space War - Space Star (32 in 1) (1988).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            ms_pacman    ROMS/Ms. Pac-Man (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        fishing_derby  ROMS/Fishing Derby (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m        crazy_climber  ROMS/Crazy Climber (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               skiing ROMS/Skiing - Le Ski (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            adventure      ROMS/Adventure (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               gopher ROMS/Gopher (Gopher Attack) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       miniature_golf ROMS/Miniature Golf - Arcade Golf (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            videocube ROMS/Atari Video Cube (Atari Cube, Video Cube) (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m          private_eye    ROMS/Private Eye (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       video_checkers ROMS/Video Checkers - Checkers - Atari Video Checkers (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math ROMS/Basic Math - Math (Math Pack) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m               tennis ROMS/Tennis - Le Tennis (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             defender       ROMS/Defender (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            up_n_down     ROMS/Up 'n Down (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 hero       ROMS/H.E.R.O. (1984).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                krull          ROMS/Krull (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           mario_bros    ROMS/Mario Bros. (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m       tic_tac_toe_3d ROMS/3-D Tic-Tac-Toe (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              othello        ROMS/Othello (1981).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                 pong ROMS/Video Olympics - Pong Sports (Paddle) (1977).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                   et ROMS/E.T. - The Extra-Terrestrial (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m             surround ROMS/Surround (32 in 1) (Bit Corporation) (R320).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           basic_math ROMS/Fun with Numbers (AKA Basic Math) (1980).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                mr_do        ROMS/Mr. Do! (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert         ROMS/Q-bert (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                qbert        ROMS/Q. Bert (1983).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m                alien          ROMS/Alien (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m           earthworld ROMS/SwordQuest - EarthWorld (Adventure I, SwordQuest I - EarthWorld) (1982).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m              bowling        ROMS/Bowling (1979).bin\n",
            "\u001b[92m[SUPPORTED]    \u001b[0m            tutankham      ROMS/Tutankham (1983).bin\n",
            "\n",
            "\n",
            "\n",
            "Imported 110 / 110 ROMs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SfKRFx8Dvit",
        "outputId": "c176f797-6e4c-4e77-fcf0-809ba7558e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#### ALE-related imports ####\n",
        "\n",
        "# Built-in libraries\n",
        "import re\n",
        "import sys\n",
        "import timeit\n",
        "import base64\n",
        "import pickle\n",
        "import logging\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Pypi libraries\n",
        "import gym\n",
        "import torch\n",
        "# import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ale_py import ALEInterface, SDL_SUPPORT\n",
        "import ale_py.roms as ROMS\n",
        "\n",
        "# Episode display\n",
        "from PIL import Image\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "# Configuration\n",
        "CUDA = 'cuda' if torch.cuda.device_count() else 'cpu'\n",
        "CPU = 'cpu'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
            "  for external in metadata.entry_points().get(self.group, []):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnvRecorder:\n",
        "    def __init__(self, env, out_dir='./results'):\n",
        "        self.out_dir = Path(out_dir)\n",
        "        self.out_dir.mkdir(exist_ok=True)\n",
        "        self.out_dir = self.out_dir.resolve()\n",
        "        self.env = env\n",
        "\n",
        "        # Workspace variables\n",
        "        self.__frame_count_padding = 0\n",
        "        self.__timestep = 0\n",
        "    \n",
        "    def __call__(self, choose_action=None, max_steps=-1, height=300):\n",
        "        self.record_episode_and_show(choose_action=choose_action, max_steps=max_steps, height=height)\n",
        "    \n",
        "    def record_episode_and_show(self, choose_action=None, max_steps=-1, height=300):\n",
        "        self.record_episode(choose_action=choose_action, max_steps=max_steps)\n",
        "        self.show_recording(height=height)\n",
        "    \n",
        "    def record_episode(self, choose_action=None, max_steps=-1):\n",
        "        \"\"\"Generate an episode and record it as record.mp4\n",
        "        Args:\n",
        "        choose_action -- callable method that returns the next action based on the current observation\n",
        "        max_steps     -- maximum number of steps after which force end episode\n",
        "        \"\"\"\n",
        "        choose_action = choose_action or (lambda _: self.env.action_space.sample())\n",
        "\n",
        "        self._clear_recording()\n",
        "        self.__timestep, done, observation =  0, False, self.env.reset()\n",
        "        self._record_frame()\n",
        "        while not done and self.__timestep != max_steps:\n",
        "            print(f'\\rRecording episode, timestep {self.__timestep+1}...', end='')\n",
        "            action = choose_action(observation)\n",
        "            observation, _, done, _ = self.env.step(action)\n",
        "            self.__timestep += 1\n",
        "            self._record_frame()\n",
        "\n",
        "        if self.__timestep == max_steps and not done:\n",
        "            print('\\nWarning: `max_steps` reached before episode terminated')\n",
        "        else:\n",
        "            print()\n",
        "\n",
        "        self._export_as_mp4()\n",
        "\n",
        "    def show_recording(self, height=300):\n",
        "        \"\"\"Show a .mp4 video in html format of the recorded episode\"\"\"\n",
        "        filepath = self.out_dir.joinpath('record.mp4')\n",
        "        video_b64 = base64.b64encode(filepath.read_bytes())\n",
        "        html = f'''<video alt=\"{filepath}\" autoplay loop controls style=\"height:{height}px\">\n",
        "                        <source src=\"data:video/mp4;base64,{video_b64.decode('ascii')}\" type=\"video/mp4\" />\n",
        "                   </video>'''\n",
        "        ipythondisplay.display(ipythondisplay.HTML(data=html))\n",
        "\n",
        "    def _clear_recording(self):\n",
        "        # This is a new episode, delete previously recorded steps\n",
        "        self.out_dir.joinpath('record').mkdir(exist_ok=True)\n",
        "        for step_png in self.out_dir.glob('record/step_*.png'):\n",
        "            step_png.unlink()\n",
        "        if self.out_dir.joinpath('record.mp4').exists():\n",
        "            self.out_dir.joinpath('record.mp4').unlink()\n",
        "    \n",
        "    def _record_frame(self):\n",
        "        # Record current timestep png\n",
        "        img = Image.fromarray(env.render('rgb_array'))\n",
        "        out_path = self.out_dir.joinpath(f'record/step_{self.__timestep}.png')\n",
        "        img.save(str(out_path))\n",
        "\n",
        "    def _export_as_mp4(self):\n",
        "        \"\"\"Convert the recorded set of png files into an mp4 video\"\"\"\n",
        "        self._standardize_frame_count_padding()\n",
        "        in_dir = self.out_dir.joinpath('record')\n",
        "        in_pattern = f'step_%0{self.__frame_count_padding}d.png'\n",
        "        out_file = self.out_dir.joinpath('record.mp4')\n",
        "        !cd $in_dir; ffmpeg -hide_banner -loglevel error -r 60 -i $in_pattern -vcodec libx264 -crf 25 -pix_fmt yuv420p -y $out_file\n",
        "    \n",
        "    def _standardize_frame_count_padding(self):\n",
        "        self.__frame_count_padding = len(str(self.__timestep))\n",
        "        number_pattern = re.compile('\\d+')\n",
        "        png_abs_glob = 'step_*.png'\n",
        "        for png_path in self.out_dir.joinpath('record').glob(png_abs_glob):\n",
        "            ts = int(number_pattern.search(png_path.stem).group(0))\n",
        "            new_name = png_path.parent.joinpath(f'step_{ts:0{self.__frame_count_padding}d}.png')\n",
        "            png_path.rename(new_name)"
      ],
      "metadata": {
        "id": "ZFF_G0wToLuO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAs6_UzuKGjz"
      },
      "source": [
        "class features:\n",
        "    @staticmethod\n",
        "    def basic(frame, palette, background, crop_size=torch.Tensor([15,10]), flatten=True):\n",
        "        # For each color in palette, tell if each pixel is that color (e.g. 210,160,128)\n",
        "        colors_in_pixels = ((frame-background).unsqueeze(-2) == palette).all(-1)\n",
        "        # Split the image into n tiles, each with dimension `crop_size` (e.g. 14,16,15,10,128)\n",
        "        cropped_colors_in_pixels = torch.stack(torch.stack(colors_in_pixels.split(crop_size[1],dim=-2)).split(crop_size[0],dim=-3))\n",
        "        # Apply logical or inside each cropped image (e.g. 14,16,128)\n",
        "        cropped_features = cropped_colors_in_pixels.any(3).any(2)\n",
        "        # Flatten the features (e.g. 28672)\n",
        "        return cropped_features.flatten() if flatten else cropped_features\n",
        "    \n",
        "    @staticmethod\n",
        "    def b_pros(frame, palette, background, crop_size=torch.Tensor([15,10])):\n",
        "        raise NotImplementedError()\n",
        "        basic_features = features.basic(frame, palette, background, crop_size=crop_size)\n",
        "        b_pros_features = torch.combinations(basic_features)\n",
        "        return b_pros_features\n",
        "    \n",
        "    @staticmethod\n",
        "    def discretized_float(number, low, high, n_intervals):\n",
        "        feature = torch.zeros(n_intervals+2, dtype=torch.bool)\n",
        "        if number < low:\n",
        "            feature[0] = True\n",
        "        elif number >= high:\n",
        "            feature[-1] = True\n",
        "        else:\n",
        "            interval = (high-low)/n_intervals\n",
        "            index = int((number-low)/interval)\n",
        "            feature[index+1] = True\n",
        "        return feature"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryEnv:\n",
        "    def __init__(self, env_name, n_features, device=CUDA, *args, **kwargs):\n",
        "        self.env = gym.make(env_name, *args, **kwargs)\n",
        "        self.n_features = int(n_features/self.env.observation_space.shape[0])\n",
        "        self.device = device\n",
        "\n",
        "        self.action_space = self.env.action_space\n",
        "        self.observation_space = self.env.observation_space\n",
        "    \n",
        "    def reset(self, *args, **kwargs):\n",
        "        observation = self.env.reset(*args, **kwargs)\n",
        "        return self._binarize_observation(observation)\n",
        "\n",
        "    def step(self, *args, **kwargs):\n",
        "        observation, reward, done, info = self.env.step(*args, **kwargs)\n",
        "        observation = self._binarize_observation(observation)\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def render(self, *args, **kwargs):\n",
        "        return self.env.render(*args, **kwargs)\n",
        "    \n",
        "    def _binarize_observation(self, observation):\n",
        "        bin_features = []\n",
        "        lows, highs = self.env.observation_space.low, self.env.observation_space.high\n",
        "        lows, highs = [-2] * 4, [2] * 4\n",
        "        for i,obs in enumerate(observation):\n",
        "            bin_features.append(features.discretized_float(obs, lows[i], highs[i], n_intervals=self.n_features))\n",
        "        return torch.cat(bin_features).to(self.device)"
      ],
      "metadata": {
        "id": "hmZZlTI8gDia"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0fNQ7lIlEdl"
      },
      "source": [
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "class EnvALE:\n",
        "    def __init__(self, rom, out_dir='results', seed=0, feature_type='ScreenRGB',\n",
        "                 regen_bg=False, bg_samples=18000, device=CUDA):\n",
        "        self.rom = rom\n",
        "        self.rom_name = rom.stem\n",
        "        self.feature_type = feature_type\n",
        "        self.device = device\n",
        "\n",
        "        self.out_dir = Path(out_dir)\n",
        "        self.out_dir.mkdir(exist_ok=True)\n",
        "        self.out_dir = self.out_dir.resolve()\n",
        "\n",
        "        # ALE\n",
        "        self.ale = ALEInterface()\n",
        "        self.ale.setInt(\"random_seed\", seed)\n",
        "        self.ale.loadROM(rom)\n",
        "\n",
        "        # gym action_space compatibility\n",
        "        action_set = self.ale.getMinimalActionSet()\n",
        "        self.action_space = Discrete(len(action_set))\n",
        "        self.action_space.action_set = action_set\n",
        "\n",
        "        # color palette\n",
        "        self.color_palette = self._get_color_palette().to(self.device)\n",
        "\n",
        "        self._bg_path = Path(f'./backgrounds/{self.rom_name}.pickle')\n",
        "        if regen_bg or not self._bg_path.exists() or not self._bg_path.is_file():\n",
        "            self.background = self._get_background(n_samples=bg_samples)\n",
        "        else:\n",
        "            with open(self._bg_path, 'rb') as file:\n",
        "                self.background = pickle.load(file).to(self.device)\n",
        "        \n",
        "        self._set_observe_method(feature_type)\n",
        "        self.observation_space = Box(low=0, high=1, shape=self._observe().shape, dtype=bool)\n",
        "\n",
        "    def reset(self, do_record=False):\n",
        "        self.ale.reset_game()\n",
        "        observation = self._observe()\n",
        "        \n",
        "        return observation\n",
        "        \n",
        "    def step(self, action):\n",
        "        if isinstance(action, int):\n",
        "            action = self.action_space.action_set[action]\n",
        "\n",
        "        reward = self.ale.act(action)\n",
        "        observation = self._observe()\n",
        "        done = self.ale.game_over()\n",
        "                \n",
        "        return observation, reward, done, None\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        if mode == 'rgb_array':\n",
        "            return self.ale.getScreenRGB()\n",
        "        else:\n",
        "            raise ValueError(f'render mode `{mode}` is not supported')\n",
        "\n",
        "    def _set_observe_method(self, feature_type):\n",
        "        if feature_type == 'ScreenRGB':\n",
        "            self._observe = lambda: torch.from_numpy(self.ale.getScreenRGB()).to(self.device)\n",
        "        elif feature_type == 'ScreenGrayscale':\n",
        "            self._observe = lambda: torch.from_numpy(self.ale.getScreenGrayscale()).to(self.device)\n",
        "        elif feature_type == 'Basic':\n",
        "            self._observe = lambda: features.basic(frame=torch.from_numpy(self.ale.getScreenRGB()).to(self.device),\n",
        "                                                   palette=self.color_palette,\n",
        "                                                   background=self.background)\n",
        "        elif feature_type == 'B-PROS':\n",
        "            self._observe = lambda: features.b_pros(frame=torch.from_numpy(self.ale.getScreenRGB()).to(self.device),\n",
        "                                                    palette=self.color_palette,\n",
        "                                                    background=self.background)\n",
        "        else:\n",
        "            raise NotImplementedError(f'Feature type `{feature_type}` is not supported')\n",
        "        \n",
        "    def _observe(self):\n",
        "        raise NotImplementedError()\n",
        "    \n",
        "    def _get_color_palette(self):\n",
        "        result = subprocess.run(['python', '-c', f'__import__(\"ale_py\").ALEInterface().loadROM(\"{str(self.rom)}\")'], capture_output=True)\n",
        "        palette_name = result.stderr.decode().splitlines()[6].strip().split()[-1]\n",
        "        with open(f'palettes/{palette_name}_Palette.pickle', 'rb') as file:\n",
        "            palette = pickle.load(file)\n",
        "        return palette\n",
        "    \n",
        "    def _get_background(self, n_samples):\n",
        "        bg_feature_type = 'ScreenRGB' if self.feature_type not in ['ScreenGrayscale',] else 'ScreenGrayscale'\n",
        "        self._set_observe_method(bg_feature_type)\n",
        "        \n",
        "        sample_i = 0\n",
        "        pixel_histogram = torch.zeros((*self.ale.getScreenDims(), self.color_palette.shape[0]), dtype=torch.int32).to(self.device)\n",
        "        while sample_i < n_samples:\n",
        "            done, observation = False, self.reset()\n",
        "            while not done and sample_i < n_samples:\n",
        "                if not sample_i%10:\n",
        "                    print(f'\\rGenerating background... {sample_i}/{n_samples} samples ({sample_i/n_samples:.0%})', end='')\n",
        "                action = self.action_space.sample()\n",
        "                observation, reward, done, info = self.step(action)\n",
        "                colors_in_pixels = (observation.unsqueeze(-2) == self.color_palette).all(-1)\n",
        "                pixel_histogram += colors_in_pixels\n",
        "                sample_i += 1\n",
        "        print('\\r', end='')\n",
        "        background_ids = pixel_histogram.argmax(axis=-1)\n",
        "        background = self.color_palette[background_ids]\n",
        "        \n",
        "        self._bg_path.parent.mkdir(exist_ok=True)\n",
        "        with open(self._bg_path, 'wb') as file:\n",
        "            pickle.dump(background.cpu(), file)\n",
        "        \n",
        "        return background"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "if print(\"Skipped background regeneration.\"):\n",
        "# if True:\n",
        "\n",
        "    from ale_py.roms import *\n",
        "    games_to_generate_bg = [Breakout, MontezumaRevenge, Venture, Qbert, Frostbite, Freeway]\n",
        "\n",
        "    for game in games_to_generate_bg:\n",
        "        print(f'\\n{game.stem}')\n",
        "        env = EnvALE(game, regen_bg=True, bg_samples=100)\n",
        "        bg_np = env.background.cpu().to(torch.uint8).numpy()\n",
        "        display(Image.fromarray(bg_np))"
      ],
      "metadata": {
        "id": "-jjm2aZ9oMGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d95a57e3-8959-4278-f6ed-a4648d38db80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped background regeneration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "if print(\"Skipped displaying stored backgrounds to reduce ouptuts.\"):\n",
        "# if True:\n",
        "\n",
        "    for filepath in Path('backgrounds').iterdir():\n",
        "        print(f'\\nBackground in `{filepath.resolve()}`')\n",
        "        with open(filepath, 'rb') as file:\n",
        "            bg = pickle.load(file)\n",
        "        display(Image.fromarray(bg.to(torch.uint8).numpy()))"
      ],
      "metadata": {
        "id": "DT3TM-u1ISRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9a947d-fff0-46a0-e051-0e497fb59061"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped displaying stored backgrounds to reduce ouptuts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh8WdcHh4Z8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e08edbd-2964-4f9a-e9ed-a2e9b48d4ced"
      },
      "source": [
        "#@title\n",
        "\n",
        "if print(\"Skipped manual test.\"):\n",
        "# if True:\n",
        "\n",
        "    env = EnvALE(ROMS.Breakout)\n",
        "    recorder = EnvRecorder(env)\n",
        "    recorder()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped manual test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SarsaPhiEBAgent:\n",
        "    def __init__(self, env, alpha=0.5, gamma=0.99, lam=0.9, beta=0.05, init_action=1, step_repeat_count=1, device=CUDA, debug=False):\n",
        "        \"\"\"An agent using Sarsa(lambda) algorithm with:\n",
        "            - Linear Function Approximation (SGD)\n",
        "            - Replacing Traces\n",
        "            - Exploration-Bonus\n",
        "        \n",
        "        Args:\n",
        "                      env -- gym-like environment\n",
        "                    alpha -- step size\n",
        "                    gamma -- discount factor\n",
        "                      lam -- trace decay\n",
        "                     beta -- exploration bonus parameter\n",
        "              init_action -- action to take after an environment reset\n",
        "        step_repeat_count -- numer of times to repeat an action every timestep (under-the-hood) \n",
        "                   device -- device on which to store tensors\n",
        "                    debug -- enable debug output\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.lam = lam\n",
        "        self.beta = beta\n",
        "        self.init_action = init_action\n",
        "        self.step_repeat_count = step_repeat_count\n",
        "        self.device = device\n",
        "        self.epsilon = 0.1 #hardcoded\n",
        "\n",
        "        self.__log = logging.Logger(name=self.__class__.__name__,\n",
        "                                    level=(logging.DEBUG if debug else logging.WARNING))\n",
        "\n",
        "        # Workspace variables\n",
        "        self.feature_space_shape = (self.env.action_space.n, *self.env.reset().shape)\n",
        "        self.weights = torch.zeros(self.feature_space_shape).to(self.device)\n",
        "        self._counts = torch.zeros(self.feature_space_shape).to(self.device)\n",
        "        self._total_steps = 0\n",
        "\n",
        "    def learn(self, n_steps):\n",
        "        start_time, done = timeit.default_timer(), True\n",
        "\n",
        "        rewards = [0]\n",
        "        episode_count = 0\n",
        "        for i in range(n_steps):\n",
        "            # Information display\n",
        "            avg_time = (timeit.default_timer()-start_time)/(i or 1)\n",
        "            print(f'\\rSarsaPhiEB learning iteration {self._total_steps+1}/{self._total_steps-i+n_steps} ({1000*avg_time:.2f}ms/iter, remaining {(n_steps-i)*avg_time:.0f}s)    ', end='')\n",
        "            # Reset env when episode ends\n",
        "            if done:\n",
        "                episode_count += 1\n",
        "                rewards.append(0)\n",
        "                next_phi = self.env.reset()\n",
        "                if self.init_action is not None:\n",
        "                    next_phi, _, _, _ = self.env.step(self.init_action)\n",
        "                next_action = self._choose_action(next_phi)\n",
        "                traces = torch.zeros(self.feature_space_shape).to(self.device)\n",
        "\n",
        "            # Advance to next timestep\n",
        "            phi, action = next_phi, next_action\n",
        "            active_features = phi.nonzero()\n",
        "            self._total_steps += 1\n",
        "\n",
        "            # Take an action\n",
        "            next_phi, reward, done, _ = self._step_repeat(action)\n",
        "            next_action = self._choose_action(next_phi)\n",
        "            rewards[episode_count] += reward\n",
        "\n",
        "            # Apply exploration bonus\n",
        "            self._counts[action,active_features] += 1\n",
        "            # reward += self._calc_exploration_bonus(phi, action)\n",
        "\n",
        "            # RL Algorithm : Sarsa(lambda) LFA(SGD) Replacing Traces\n",
        "            traces *= self.gamma *  self.lam\n",
        "            traces[action, active_features] = 1 # phi[active_features].to(traces.dtype)\n",
        "\n",
        "            if not done:\n",
        "                delta = reward + self.gamma * self._action_value(next_phi, next_action) - self._action_value(phi, action)\n",
        "            else:\n",
        "                # In terminal state, all state-action values are 0\n",
        "                delta = reward - self._action_value(phi, action)\n",
        "            self.weights += self.alpha * delta * traces\n",
        "\n",
        "        print(f'\\nTotal elapsed time: {datetime.utcfromtimestamp(timeit.default_timer()-start_time).strftime(\"%H:%M:%S.%f\")}')\n",
        "        return rewards\n",
        "    \n",
        "    def _step_repeat(self, action):\n",
        "        for i in range(self.step_repeat_count):\n",
        "            observation, reward, done, info = self.env.step(action)\n",
        "            if done: break\n",
        "        return observation, reward, done, info\n",
        "    \n",
        "    def _action_value(self, phi, action):\n",
        "        return (self.weights[action]@phi.to(self.weights.dtype)).item()\n",
        "    \n",
        "    def _choose_action(self, phi):\n",
        "        if torch.rand(1) > self.epsilon:\n",
        "            return (self.weights@phi.to(self.weights.dtype)).argmax().item()\n",
        "        else:\n",
        "            return self.env.action_space.sample()\n",
        "    \n",
        "    def _calc_exploration_bonus(self, phi, action):\n",
        "        # Compute the exploration bonus\n",
        "        phi_occ = torch.cat((self._counts[action, phi], self._total_steps-self._counts[action, ~phi])).to(self.device)\n",
        "        rho = ((phi_occ+1/2) / (self._total_steps+1)).prod()\n",
        "        rho_prime = ((phi_occ+1+1/2) / (self._total_steps+1+1)).prod()\n",
        "        pseudocount = (rho*(1-rho_prime)) / (rho_prime-rho)\n",
        "        exploration_bonus = self.beta / pseudocount.sqrt()\n",
        "        return exploration_bonus"
      ],
      "metadata": {
        "id": "5kQC-tTUHgFl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = BinaryEnv('CartPole-v1', 1000)\n",
        "#env = EnvALE(ROMS.Breakout, feature_type='Basic')\n",
        "\n",
        "agent = SarsaPhiEBAgent(env, init_action=0)\n",
        "recorder = EnvRecorder(env)\n",
        "\n",
        "rew = agent.learn(8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyjGd3iWHV6H",
        "outputId": "ee11e2dc-dab7-42f1-f895-6a0314516700"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SarsaPhiEB learning iteration 8000/8000 (1.63ms/iter, remaining 0s)    \n",
            "Total elapsed time: 00:00:13.005189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(rew)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "bGAmCpNYER7W",
        "outputId": "e0dba794-8400-4cab-91b7-c9e157cf1afc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recorder(choose_action=agent._choose_action, max_steps=1000)"
      ],
      "metadata": {
        "id": "91iMb12MmrSG",
        "outputId": "080cdb31-0f26-4dc0-b9a9-e2b91cf53150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in float_scalars\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rRecording episode, timestep 1...\rRecording episode, timestep 2...\rRecording episode, timestep 3...\rRecording episode, timestep 4...\rRecording episode, timestep 5...\rRecording episode, timestep 6...\rRecording episode, timestep 7...\rRecording episode, timestep 8...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<video alt=\"/content/results/record.mp4\" autoplay loop controls style=\"height:300px\">\n",
              "                        <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
              "                   </video>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "# Copyright (C)                                                       #\n",
        "# 2017-2018 Shangtong Zhang(zhangshangtong.cpp@gmail.com)             #\n",
        "# Permission given to modify the code as long as you keep this        #\n",
        "# declaration at the top                                              #\n",
        "#######################################################################\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib\n",
        "# matplotlib.use('Agg')\n",
        "# import matplotlib.pyplot as plt\n",
        "# from math import floor\n",
        "# from tqdm import tqdm\n",
        "\n",
        "\n",
        "# all possible actions\n",
        "ACTIONS = range(4)\n",
        "\n",
        "# discount is always 1.0 in these experiments\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "# use optimistic initial value, so it's ok to set epsilon to 0\n",
        "EPSILON = 0.01\n",
        "\n",
        "# maximum steps per episode\n",
        "STEP_LIMIT = 5000\n",
        "\n",
        "\n",
        "# get action at @position and @velocity based on epsilon greedy policy and @valueFunction  #########################    use our own get_action. modified it, may work as intended\n",
        "def get_action(observation, valueFunction):\n",
        "    if np.random.binomial(1, EPSILON) == 1:\n",
        "        return np.random.choice(ACTIONS)\n",
        "    values = []\n",
        "    for action in ACTIONS:\n",
        "        values.append(valueFunction.value(observation))  \n",
        "    maxi = np.max(values)\n",
        "    bestactions = np.where(values==maxi,1.0,0.0)\n",
        "    for i in range(len(ACTIONS)):\n",
        "      bestactions[i] = bestactions[i]*np.random.uniform()\n",
        "    action = np.argmax(bestactions)\n",
        "    return action\n",
        "\n",
        "\n",
        "\n",
        "# replacing trace update rule\n",
        "# @trace: old trace (will be modified)\n",
        "# @activeTiles: current active tile indices\n",
        "# @lam: lambda\n",
        "# @return: new trace for convenience\n",
        "def replacing_trace(trace, activeTiles, lam):\n",
        "    active = (torch.arange(len(trace)).to(device)[None,...] == activeTiles.flatten()[...,None]).any(0)\n",
        "    trace[active] = 1\n",
        "    trace[~active] *= lam * DISCOUNT\n",
        "    return trace\n",
        "\n",
        "\n",
        "\n",
        "# wrapper class for Sarsa(lambda)\n",
        "class Sarsa:\n",
        "    # In this example I use the tiling software instead of implementing standard tiling by myself\n",
        "    # One important thing is that tiling is only a map from (state, action) to a series of indices\n",
        "    # It doesn't matter whether the indices have meaning, only if this map satisfy some property\n",
        "    # View the following webpage for more information\n",
        "    # http://incompleteideas.net/sutton/tiles/tiles3.html\n",
        "    # @maxSize: the maximum # of indices\n",
        "    #the hashing is a lfa?\n",
        "    def __init__(self, step_size, lam, trace_update=replacing_trace, max_size=28672, initial_weights=0):\n",
        "        self.max_size = max_size\n",
        "        self.trace_update = trace_update\n",
        "        self.lam = lam\n",
        "\n",
        "        # divide step size equally to each tiling\n",
        "        self.step_size = step_size/10\n",
        "\n",
        "        # weight for each tile\n",
        "        if initial_weights == 0:\n",
        "          self.weights =torch.zeros(max_size).to(device) #max size is the number of features?\n",
        "        else:\n",
        "          self.weights = initial_weights.to(device)\n",
        "\n",
        "        # trace for each tile\n",
        "        self.trace = torch.zeros(max_size).to(device)\n",
        "\n",
        "\n",
        "\n",
        "    # estimate the value of given state and action\n",
        "    def value(self, observation):\n",
        "        active_tiles = np.nonzero(observation)\n",
        "        return self.weights[active_tiles].sum()\n",
        "\n",
        "    # learn with given state, action and target\n",
        "    def learn(self, observation, target):\n",
        "        active_tiles = np.nonzero(observation)\n",
        "        estimation = self.weights[active_tiles].sum()\n",
        "        delta = target - estimation\n",
        "        #print('estimation array: ' + str(self.weights[active_tiles]))\n",
        "        # print('estimation: ' + str(self.weights[active_tiles].sum()))\n",
        "        if self.trace_update == replacing_trace:\n",
        "            self.trace_update(self.trace, active_tiles, self.lam)\n",
        "        else:\n",
        "            raise Exception('Unexpected Trace Type')\n",
        "        self.weights += self.step_size * delta * self.trace\n",
        "        # print('delta: ' + str(delta))\n",
        "        # print('weights: ' +  str(self.weights))\n",
        "\n",
        "\n",
        "# play Mountain Car for one episode based on given method `evaluator`\n",
        "# return: total steps in this episode\n",
        "def play(evaluator, env):\n",
        "\n",
        "    action = random.choice(ACTIONS)\n",
        "    steps = 0\n",
        "    while True:\n",
        "        next_observation, reward, done, info = env.step(action)\n",
        "        next_action = get_action(next_observation, evaluator)    #########################    use our own get_action  ??? modified it, may work as intented\n",
        "        steps += 1\n",
        "        target = reward + DISCOUNT * evaluator.value(next_observation)          ############# use our own value function ??? modified it, may work as intented\n",
        "        evaluator.learn(observation, target)\n",
        "        observation = next_observation\n",
        "        action = next_action\n",
        "        if done:\n",
        "            break\n",
        "        if steps >= STEP_LIMIT:\n",
        "            print('Step Limit Exceeded!')\n",
        "            break\n",
        "    return steps\n",
        "\n",
        "class BaseAgent:\n",
        "  \"\"\" The base agent class function.\n",
        "  \"\"\"\n",
        "  def __init__(self, nb_features=28672):\n",
        "    #nothing for now\n",
        "    self.gamma = 1\n",
        "    self.features = nb_features\n",
        "    self.rhos = torch.ones(self.features).to(device) #stores the rho_i values\n",
        "\n",
        "\n",
        "  def takeAction(self, t):\n",
        "    phis = [[0,1,0],[0,1,0],[0,1,0],[1,0,1]]\n",
        "    return phis[t]\n",
        "\n",
        "\n",
        "  def updateRho_i(self, counts, t):\n",
        "    M = self.features\n",
        "    self.rhos = (counts+1.5)/(t+1)\n",
        "    return 0\n",
        "\n",
        "\n",
        "  def PHI_EB(self, evaluator, env, beta=0.05, t_end=200):\n",
        "    t = 0\n",
        "    M = self.features #number of features\n",
        "    counts = torch.zeros(M).to(device)\n",
        "    states = torch.zeros(t_end,M).to(device) #stores the previous phis for all timesteps\n",
        "\n",
        "    action = 1 #starting the game for the agent on the first game\n",
        "    old_phi = env._observe()\n",
        "    print('starting iterations')\n",
        "    print('rhos: ' + str(self.rhos))\n",
        "    while t < t_end:\n",
        "    #   print(\"Iteration #{}\".format(t))\n",
        "      #observe phi(s), reward\n",
        "    #   phi, reward, done, info = obs.clone(), 0, False, None\n",
        "      phi, reward, done, info = env.step(action)\n",
        "    #   print(phi.shape)\n",
        "    #   print('--------------------------------------------------------------')\n",
        "    #   print('took action: ', env.action_space[action])\n",
        "      next_action = get_action(phi, evaluator)\n",
        "    #   print('phi: ' + str(phi))\n",
        "      \n",
        "      #compute rho_t(phi) (feature visit-density)\n",
        "      if t > 0:\n",
        "        counts = (phi==states[0:t]).sum(0)\n",
        "        # print(counts)\n",
        "        self.rhos = (counts+0.5)/(t+1)\n",
        "        # print('rhos: ' + str(self.rhos))\n",
        "        rho_t = torch.prod(self.rhos)\n",
        "      else:\n",
        "        rho_t = 0.5**M\n",
        "    #   print('rho_t ' + str(rho_t))\n",
        "      #update all rho_i with observed phi\n",
        "      states[t] = phi\n",
        "      self.updateRho_i(counts, t+1)\n",
        "    #   print('min rho_i_t: ' + str(min(self.rhos)))\n",
        "      \n",
        "      #compute rho_t+1(phi)\n",
        "      new_rho_t = 1\n",
        "      # THIS IS A BOTTLENECK (tested in CPU mode: 74ms -> 178ms)\n",
        "      for i in range(M):\n",
        "        new_rho_t = new_rho_t*self.rhos[i]\n",
        "      if new_rho_t <= 1e-323: #this is to avoid division by zero, might need to be tweaked\n",
        "        new_rho_t = 1e-323\n",
        "    #   print('new_rho_t ' + str(new_rho_t))\n",
        "\n",
        "      #compute Nhat_t(s)\n",
        "      Nhat_t = rho_t*(1-new_rho_t)/(new_rho_t-rho_t)\n",
        "    #   print('Nhat_t: ',   Nhat_t)\n",
        "      if Nhat_t <= 1e-323: #this is to avoid division by zero again, might need to be tweaked\n",
        "        Nhat_t = torch.tensor([1e-323]).to(device)\n",
        "\n",
        "      #compute R(s,a) (empirical reward)\n",
        "      explorationBonus = beta/torch.sqrt(Nhat_t)\n",
        "      if torch.isnan(explorationBonus) or explorationBonus >= 1e3:\n",
        "        explorationBonus = 1e3\n",
        "\n",
        "      reward = reward + explorationBonus\n",
        "    #   print('reward: ',reward)\n",
        "\n",
        "    #   print('state value: ' + str(evaluator.value(phi)))\n",
        "      #pass phi(s) and reward to RL algo to update theta_t\n",
        "      target = reward + self.gamma * evaluator.value(phi)          ############# use our own value function ??? modified it, may work as intented\n",
        "      # THIS IS A BOTTLENECK (tested in CPU mode: 190ms -> 207ms)\n",
        "      evaluator.learn(old_phi, target)\n",
        "\n",
        "      if done:\n",
        "        #break\n",
        "        env.reset()\n",
        "        action = 1\n",
        "        old_phi = env._observe()\n",
        "        print('episode ended on step ', t, 'starting a new one')\n",
        "      else:\n",
        "        old_phi = phi\n",
        "        action = next_action\n",
        "      t += 1\n",
        "      continue\n",
        "\n",
        "\n",
        "    return evaluator.weights\n",
        "\n",
        "# from ale_py.roms import Breakout\n",
        "# import timeit\n",
        "# env = EnvALE(Breakout, feature_type='Basic')\n",
        "# print(env.action_space)\n",
        "# alpha = 0.5\n",
        "# lam = 0.9\n",
        "# #we can upload previous weights as as tensor, or initialize at 0\n",
        "# previous_weights = 0\n",
        "# obs = torch.randint(0,2,(28672,), dtype=bool).to(device)\n",
        "# t_end = 10\n",
        "\n",
        "# evaluator = Sarsa(alpha, lam, replacing_trace, 28672, previous_weights)\n",
        "# agent = BaseAgent()\n",
        "# env.reset(do_record=False)\n",
        "# start_time = timeit.default_timer()\n",
        "# weights = agent.PHI_EB(evaluator, env, beta=0.05, t_end=t_end)\n",
        "# print(f'{1000*(timeit.default_timer()-start_time)/t_end}ms per timestep')"
      ],
      "metadata": {
        "id": "6OmpMlK6HrOl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}